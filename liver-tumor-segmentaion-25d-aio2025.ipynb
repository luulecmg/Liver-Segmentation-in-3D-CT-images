{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":110174,"databundleVersionId":13423659,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13812427,"sourceType":"datasetVersion","datasetId":8795147},{"sourceId":13815676,"sourceType":"datasetVersion","datasetId":8797652},{"sourceId":13826771,"sourceType":"datasetVersion","datasetId":8805635},{"sourceId":657316,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":496879,"modelId":512245},{"sourceId":657915,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":496887,"modelId":512251}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Overview pipeline","metadata":{}},{"cell_type":"code","source":"# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n# ‚îÇ  CT Volume (512, 512, Z)             ‚îÇ\n# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n#                ‚îÇ\n#                ‚îÇ Loop over each slice\n#                ‚ñº\n# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n# ‚îÇ  Slice 2D ‚Üí Resize ‚Üí Predict         ‚îÇ\n# ‚îÇ  Repeat Z times                      ‚îÇ\n# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n#                ‚îÇ\n#                ‚îÇ Stack predictions\n#                ‚ñº\n# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n# ‚îÇ  Prediction Volume (512, 512, Z)     ‚îÇ ‚Üê 3D VOLUME\n# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n#                ‚îÇ\n#                ‚îÇ POST-PROCESSING 3D:\n#                ‚îÇ 1. Remove small objects\n#                ‚îÇ 2. Keep largest component\n#                ‚îÇ 3. Fill holes\n#                ‚ñº\n# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n# ‚îÇ  Cleaned Volume (512, 512, Z)        ‚îÇ\n# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n#                ‚îÇ\n#                ‚îÇ Loop over each slice\n#                ‚ñº\n# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n# ‚îÇ  RLE encode each slice               ‚îÇ\n# ‚îÇ  ‚Üí submission.csv                    ‚îÇ\n# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Import Libraries & Configre parameters","metadata":{}},{"cell_type":"markdown","source":"## 1.1. Import libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q segmentation_models_pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T13:42:56.618864Z","iopub.execute_input":"2025-11-23T13:42:56.619497Z","iopub.status.idle":"2025-11-23T13:44:11.793124Z","shell.execute_reply.started":"2025-11-23T13:42:56.619459Z","shell.execute_reply":"2025-11-23T13:44:11.792286Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pandas as pd\nimport nibabel as nib\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\nfrom skimage.morphology import remove_small_objects\nfrom skimage.measure import label\nfrom scipy.ndimage import binary_fill_holes\nfrom tqdm.auto import tqdm\nimport wandb\n\nimport glob\nimport os\nimport gc\nfrom datetime import datetime\nfrom time import time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"‚úÖ Import libraries completed!\")\nprint(f\"‚úÖ PyTorch version: {torch.__version__}\")\nprint(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-23T13:44:11.794673Z","iopub.execute_input":"2025-11-23T13:44:11.794965Z","iopub.status.idle":"2025-11-23T13:44:24.327563Z","shell.execute_reply.started":"2025-11-23T13:44:11.794936Z","shell.execute_reply":"2025-11-23T13:44:24.326763Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Import libraries completed!\n‚úÖ PyTorch version: 2.6.0+cu124\n‚úÖ CUDA available: True\n‚úÖ GPU: Tesla T4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 1.2. Config params","metadata":{}},{"cell_type":"code","source":"HU_MIN, HU_MAX = -100, 250  # Hounsfield Units window for liver\nIMAGE_SIZE = 512  # Resize slices to 512x512\nN_SLICES = 3  # 2.5D: use 3 consecutive slices as channels\n\n### Data paths\nIMAGE_DIR = \"/kaggle/input/aio2025liverseg/train/volume\"\nLABEL_DIR = \"/kaggle/input/aio2025liverseg/train/segmentation\"\nTEST_DIR = \"/kaggle/input/aio2025liverseg/test\"\n\n### Data split\nTRAIN_SIZE = 0.875  # 87.5% train, 12.5% val\nRANDOM_STATE = 42\n\n### Training hyperparameters\nBATCH_SIZE = 32  \nNUM_WORKERS = 4\nEPOCHS = 100\nLR = 1e-4\nWEIGHT_DECAY = 1e-4\n\n### Early stopping\nPATIENCE = 15\n\n### Model architecture\nARCHITECTURE = 'UnetPlusPlus'\nENCODER = 'efficientnet-b5'\nENCODER_WEIGHTS = 'imagenet'\n\n### Wandb config\nWANDB_ENTITY = \"levanluucmg-aio\"\nWANDB_PROJECT = \"liver_tumor_segmentation_in_3D_images_kaggle\"\nEXTRA_NOTE = \"2.5D_UNet++_EfficientNetB5_NOaugment\"\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"HYPERPARAMETERS\")\nprint(\"=\"*70)\nprint(f\"Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\nprint(f\"2.5D approach: {N_SLICES} consecutive slices\")\nprint(f\"HU window: [{HU_MIN}, {HU_MAX}]\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Learning rate: {LR}\")\nprint(f\"Max epochs: {EPOCHS}\")\nprint(f\"Model: {ARCHITECTURE}\")\nprint(f\"Encoder: {ENCODER}\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T13:44:24.328407Z","iopub.execute_input":"2025-11-23T13:44:24.328831Z","iopub.status.idle":"2025-11-23T13:44:24.337265Z","shell.execute_reply.started":"2025-11-23T13:44:24.328810Z","shell.execute_reply":"2025-11-23T13:44:24.336461Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nHYPERPARAMETERS\n======================================================================\nImage size: 512x512\n2.5D approach: 3 consecutive slices\nHU window: [-100, 250]\nBatch size: 32\nLearning rate: 0.0001\nMax epochs: 100\nModel: UnetPlusPlus\nEncoder: efficientnet-b5\n======================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 2. Data Processing","metadata":{}},{"cell_type":"markdown","source":"## 2.1. Preprocessing (offline - run 1 time only)","metadata":{}},{"cell_type":"code","source":"# def pre_extract_slices_to_disk(volume_list, output_dir, n_slices=3, min_liver_pixels=100):\n#     \"\"\"\n#     Pre-extract all valid slices and save to disk\n#     Run this ONCE before training\n    \n#     Args:\n#         volume_list: List of volume dicts\n#         output_dir: Directory to save extracted slices\n#         n_slices: Number of slices for 2.5D\n#         min_liver_pixels: Minimum liver pixels\n#     \"\"\"\n#     os.makedirs(output_dir, exist_ok=True)\n#     os.makedirs(os.path.join(output_dir, \"images\"), exist_ok=True)\n#     os.makedirs(os.path.join(output_dir, \"masks\"), exist_ok=True)\n    \n#     half_slices = n_slices // 2\n#     slice_metadata = []\n    \n#     print(f\"{'='*70}\")\n#     print(f\"PRE-EXTRACTING SLICES TO DISK: {output_dir}\")\n#     print(f\"{'='*70}\")\n    \n#     for vol_idx, vol_dict in enumerate(tqdm(volume_list, desc=\"Extracting\")):\n#         # Load volume\n#         img_nii = nib.load(vol_dict['image'])\n#         img_data = img_nii.get_fdata().astype(np.float32)\n        \n#         label_nii = nib.load(vol_dict['label'])\n#         label_data = label_nii.get_fdata().astype(np.uint8)\n        \n#         # Preprocess\n#         img_data = np.clip(img_data, HU_MIN, HU_MAX)\n#         img_data = (img_data - HU_MIN) / (HU_MAX - HU_MIN)\n#         label_data = (label_data > 0).astype(np.uint8)\n        \n#         # Extract slices\n#         n_slices_total = img_data.shape[2]\n        \n#         for z_idx in range(n_slices_total):\n#             # Filter\n#             mask_slice = label_data[:, :, z_idx]\n#             if (mask_slice > 0).sum() < min_liver_pixels:\n#                 continue\n            \n#             # Extract 2.5D\n#             slices_list = []\n#             for offset in range(-half_slices, half_slices + 1):\n#                 z = z_idx + offset\n#                 z = max(0, min(z, n_slices_total - 1))\n#                 slices_list.append(img_data[:, :, z])\n            \n#             image_25d = np.stack(slices_list, axis=-1)  # (H, W, 3)\n#             mask_2d = mask_slice\n            \n#             # Resize\n#             image_25d = cv2.resize(image_25d, (IMAGE_SIZE, IMAGE_SIZE), \n#                                   interpolation=cv2.INTER_LINEAR)\n#             mask_2d = cv2.resize(mask_2d, (IMAGE_SIZE, IMAGE_SIZE), \n#                                 interpolation=cv2.INTER_NEAREST)\n            \n#             # Save as NPZ (compressed)\n#             slice_id = f\"vol{vol_idx:03d}_slice{z_idx:03d}\"\n            \n#             img_path = os.path.join(output_dir, \"images\", f\"{slice_id}.npz\")\n#             mask_path = os.path.join(output_dir, \"masks\", f\"{slice_id}.npz\")\n            \n#             np.savez_compressed(img_path, data=image_25d)\n#             np.savez_compressed(mask_path, data=mask_2d)\n            \n#             slice_metadata.append({\n#                 'image_path': img_path,\n#                 'mask_path': mask_path,\n#                 'slice_id': slice_id\n#             })\n        \n#         del img_data, label_data\n#         gc.collect()\n    \n#     # Save metadata\n#     metadata_df = pd.DataFrame(slice_metadata)\n#     metadata_df.to_csv(os.path.join(output_dir, \"metadata.csv\"), index=False)\n    \n#     print(f\"‚úÖ Extracted {len(slice_metadata)} slices\")\n#     print(f\"‚úÖ Saved to: {output_dir}\")\n    \n#     return metadata_df\n\n\n# image_file_paths = sorted(glob.glob(f\"{IMAGE_DIR}/*.nii\"))\n# label_file_paths = sorted(glob.glob(f\"{LABEL_DIR}/*.nii\"))\n\n# # Create volume list\n# volume_files = [\n#     {\"image\": img_path, \"label\": lbl_path}\n#     for img_path, lbl_path in zip(image_file_paths, label_file_paths)\n# ]\n\n# print(f\"\\n{'='*70}\")\n# print(\"DATA LOADING\")\n# print(f\"{'='*70}\")\n# print(f\"‚úÖ Found {len(volume_files)} 3D CT volumes\")\n\n# ### Train/Val split (IMPORTANT: split by volumes, not slices!)\n# train_volumes, val_volumes = train_test_split(\n#     volume_files,\n#     train_size=TRAIN_SIZE,\n#     random_state=RANDOM_STATE,\n#     shuffle=True\n# )\n\n# print(f\"üìä Train volumes: {len(train_volumes)}\")\n# print(f\"üìä Val volumes: {len(val_volumes)}\")\n\n# # Run pre-extraction ONCE\n# print(\"\\nüîß Running pre-extraction...\")\n# train_metadata = pre_extract_slices_to_disk(\n#     volume_list=train_volumes,\n#     output_dir=\"./preprocessed_data/train\",\n#     n_slices=N_SLICES,\n#     min_liver_pixels=0 # Keep all slices for training\n# )\n\n# val_metadata = pre_extract_slices_to_disk(\n#     volume_list=val_volumes,\n#     output_dir=\"./preprocessed_data/val\",\n#     n_slices=N_SLICES,\n#     min_liver_pixels=0  # Keep all slices for validation\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T00:27:20.328447Z","iopub.execute_input":"2025-11-23T00:27:20.328627Z","iopub.status.idle":"2025-11-23T00:27:22.555985Z","shell.execute_reply.started":"2025-11-23T00:27:20.328612Z","shell.execute_reply":"2025-11-23T00:27:22.555373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil, os\n\n# shutil.make_archive(\"preprocessed_data\", \"zip\", \"/kaggle/working\", \"/kaggle/working/preprocessed_data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T00:27:22.556628Z","iopub.execute_input":"2025-11-23T00:27:22.556820Z","iopub.status.idle":"2025-11-23T00:27:22.577830Z","shell.execute_reply.started":"2025-11-23T00:27:22.556804Z","shell.execute_reply":"2025-11-23T00:27:22.577283Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2. Dataset & DataLoader","metadata":{}},{"cell_type":"code","source":"class PreExtractedDataset(Dataset):\n    \"\"\"\n    Dataset for pre-extracted slices\n    Super fast loading!\n    \"\"\"\n    def __init__(self, metadata_csv, transform=None):\n        self.metadata = pd.read_csv(metadata_csv)\n        self.transform = transform\n        self.base_dir = os.path.dirname(metadata_csv)\n        \n    def __len__(self):\n        return len(self.metadata)\n    \n    def __getitem__(self, idx):\n        row = self.metadata.iloc[idx]\n        \n        # Extract relative filenames (basename)\n        img_name = os.path.basename(row['image_path'])\n        mask_name = os.path.basename(row['mask_path'])\n\n        # Construct correct paths on Kaggle\n        image_path = os.path.join(self.base_dir, \"images\", img_name)\n        mask_path = os.path.join(self.base_dir, \"masks\", mask_name)\n\n        # Load\n        image = np.load(image_path)['data'].astype(np.float32)\n        mask = np.load(mask_path)['data'].astype(np.float32)\n\n        # Transform\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n        \n        mask = mask.unsqueeze(0)\n        return image, mask\n\ndef get_train_transforms():\n    \"\"\"Training augmentation pipeline\"\"\"\n    return A.Compose([\n        A.Resize(height=256, width=256, interpolation=cv2.INTER_LINEAR, mask_interpolation=cv2.INTER_NEAREST),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, border_mode=cv2.BORDER_CONSTANT, p=0.5),\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n        ToTensorV2()\n    ])\n\ndef get_val_transforms():\n    \"\"\"Validation transform (no augmentation)\"\"\"\n    return A.Compose([\n        A.Resize(height=256, width=256, interpolation=cv2.INTER_LINEAR, mask_interpolation=cv2.INTER_NEAREST),\n        ToTensorV2()])\n\nprint(\"‚úÖ Dataset and transforms defined!\")","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-11-23T09:48:47.013979Z","iopub.execute_input":"2025-11-23T09:48:47.014587Z","iopub.status.idle":"2025-11-23T09:48:47.023271Z","shell.execute_reply.started":"2025-11-23T09:48:47.014557Z","shell.execute_reply":"2025-11-23T09:48:47.022441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_file_paths = sorted(glob.glob(f\"{IMAGE_DIR}/*.nii\"))\nlabel_file_paths = sorted(glob.glob(f\"{LABEL_DIR}/*.nii\"))\n\n# Create volume list\nvolume_files = [\n    {\"image\": img_path, \"label\": lbl_path}\n    for img_path, lbl_path in zip(image_file_paths, label_file_paths)\n]\n\nprint(f\"\\n{'='*70}\")\nprint(\"DATA LOADING\")\nprint(f\"{'='*70}\")\nprint(f\"‚úÖ Found {len(volume_files)} 3D CT volumes\")\n\n### Train/Val split (IMPORTANT: split by volumes, not slices!)\ntrain_volumes, val_volumes = train_test_split(\n    volume_files,\n    train_size=TRAIN_SIZE,\n    random_state=RANDOM_STATE,\n    shuffle=True\n)\n\nprint(f\"üìä Train volumes: {len(train_volumes)}\")\nprint(f\"üìä Val volumes: {len(val_volumes)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:51:10.858712Z","iopub.execute_input":"2025-11-23T05:51:10.858959Z","iopub.status.idle":"2025-11-23T05:51:10.899339Z","shell.execute_reply.started":"2025-11-23T05:51:10.858940Z","shell.execute_reply":"2025-11-23T05:51:10.898577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Create datasets\ntrain_dataset = PreExtractedDataset(\n    metadata_csv=\"/kaggle/input/train-preprocess-dataset/train/metadata.csv\",\n    transform=get_train_transforms()\n)\nval_dataset = PreExtractedDataset(\n    metadata_csv=\"/kaggle/input/preprocess-2d-data/preprocessed_data/val/metadata.csv\",\n    transform=get_val_transforms()\n)\nprint(\"‚úÖ Pre-extracted datasets ready!\")\n\n### Create dataloaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,  # Already sorted by volume!\n    num_workers=NUM_WORKERS, \n    pin_memory=False,  # Set to False to save memory\n    drop_last=True,\n    persistent_workers=False \n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=False,\n    persistent_workers=False\n)\n\nprint(f\"\\n{'='*70}\")\nprint(\"DATALOADER INFO\")\nprint(f\"{'='*70}\")\nprint(f\"‚úÖ Train batches per epoch: {len(train_loader)} | Val batches: {len(val_loader)}\")\nprint(f\"‚úÖ Train slices: {len(train_dataset)} | Val slices: {len(val_dataset)}\")\nprint(f\"‚úÖ Train value range: {len(train_dataset)} | Val slices: {len(val_dataset)}\")\nprint(f\"{'='*70}\")\n\n# Force garbage collection\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T05:51:10.900161Z","iopub.execute_input":"2025-11-23T05:51:10.900531Z","iopub.status.idle":"2025-11-23T05:51:11.285225Z","shell.execute_reply.started":"2025-11-23T05:51:10.900487Z","shell.execute_reply":"2025-11-23T05:51:11.284103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Init model, optimizer, loss function","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"MODEL SETUP\")\nprint(f\"{'='*70}\")\nprint(f\"Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n\n### Define Model: UNet++ with EfficientNet-B5\nmodel = smp.UnetPlusPlus(\n    encoder_name=ENCODER,\n    encoder_weights=ENCODER_WEIGHTS, # ENCODER_WEIGHTS, if using pretrained\n    in_channels=N_SLICES,  # 2.5D: 3 slices\n    classes=1,  # Binary segmentation (liver vs background)\n    activation=None,  # I'll apply sigmoid in loss\n    decoder_attention_type=\"scse\"  # Spatial-Channel Squeeze & Excitation\n)\n\n# Multi-GPU\nif torch.cuda.device_count() > 1:\n    print(f\"‚úÖ Using {torch.cuda.device_count()} GPUs with DataParallel\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(device)\n\nprint(f\"‚úÖ Model: {ARCHITECTURE}\")\nprint(f\"‚úÖ Encoder: {ENCODER} (pretrained on {ENCODER_WEIGHTS})\")\nprint(f\"‚úÖ Decoder attention: SCSE\")\n\nclass DiceBCELoss(nn.Module):\n    def __init__(self, weight_dice=0.7, weight_bce=0.3, smooth=1e-6):\n        super().__init__()\n        self.weight_dice = weight_dice\n        self.weight_bce = weight_bce\n\n    def forward(self, inputs, targets):\n        preds = torch.sigmoid(inputs)\n        # Compute BCE loss\n        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets)\n        \n        # Compute Dice loss\n        smooth = 1e-6\n        intersection = (preds * targets).sum()\n        union = preds.sum() + targets.sum()\n        dice_loss = 1 - (2. * intersection + smooth) / (union + smooth)\n\n        # Combined the loss\n        combined_loss = self.weight_dice * dice_loss + self.weight_bce * bce_loss\n        return combined_loss\n\n\nloss_fn = DiceBCELoss(weight_dice=0.7, weight_bce=0.3)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-7, last_epoch=-1)\n\nprint(f\"‚úÖ Loss: Combined (Dice + BCE)\")\nprint(f\"‚úÖ Optimizer: AdamW (lr={LR}, weight_decay={WEIGHT_DECAY})\")\nprint(f\"‚úÖ Scheduler: CosineAnnealingLR\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T00:27:23.063086Z","iopub.execute_input":"2025-11-23T00:27:23.063333Z","iopub.status.idle":"2025-11-23T00:27:25.740518Z","shell.execute_reply.started":"2025-11-23T00:27:23.063315Z","shell.execute_reply":"2025-11-23T00:27:25.739741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install torchinfo\n# from torchinfo import summary\n\n# summary(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T00:27:25.742340Z","iopub.execute_input":"2025-11-23T00:27:25.742821Z","iopub.status.idle":"2025-11-23T00:27:29.044918Z","shell.execute_reply.started":"2025-11-23T00:27:25.742803Z","shell.execute_reply":"2025-11-23T00:27:29.044257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Training","metadata":{}},{"cell_type":"markdown","source":"## 4.1. Configure funtioncs","metadata":{}},{"cell_type":"code","source":"def dice_coefficient(pred, target, threshold=0.5, smooth=1e-6):\n    \"\"\"Calculate Dice coefficient for evaluation\"\"\"\n    pred = (torch.sigmoid(pred) > threshold).float()\n    intersection = (pred * target).sum()\n    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n    return dice.item()\n\ndef iou_score(pred, target, threshold=0.5, smooth=1e-6):\n    \"\"\"Calculate IoU/Jaccard score\"\"\"\n    pred = (torch.sigmoid(pred) > threshold).float()\n    intersection = (pred * target).sum()\n    union = pred.sum() + target.sum() - intersection\n    iou = (intersection + smooth) / (union + smooth)\n    return iou.item()\n\n# ============================================================================\n# MEMORY OPTIMIZATION UTILITIES\n# ============================================================================\ndef clear_memory():\n    \"\"\"Aggressively clear memory\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n# Monitor memory during training\nimport psutil\n\ndef monitor_memory_callback():\n    \"\"\"Callback to monitor memory\"\"\"\n    process = psutil.Process()\n    mem_gb = process.memory_info().rss / 1024**3\n    \n    if mem_gb > 28:  # Warning if > 28GB (out of 30GB)\n        print(f\"‚ö†Ô∏è  HIGH MEMORY WARNING: {mem_gb:.2f} GB\")\n        clear_memory()\n    return mem_gb\n\n# ============================================================================\n# Training & Validation step\n# ============================================================================\ndef train_step_optimized(model, loader, loss_fn, optimizer, scaler, device, epoch, accumulation_steps=4):\n    \"\"\"Training with memory monitoring\"\"\"\n    model.train()\n    \n    running_loss = 0.0\n    running_dice = 0.0\n    running_iou = 0.0\n    num_batches = 0\n\n    optimizer.zero_grad(set_to_none=True)\n    \n    pbar = tqdm(loader, desc=f\"[Epoch {epoch+1}/{EPOCHS}] Training\")\n    \n    for batch_idx, (images, masks) in enumerate(pbar):\n        images = images.to(device, non_blocking=True)\n        masks = masks.to(device, non_blocking=True)\n        \n        with autocast():\n            outputs = model(images)\n            loss = loss_fn(outputs, masks)\n            loss = loss / accumulation_steps\n        \n        scaler.scale(loss).backward()\n        if (batch_idx+1) % accumulation_steps == 0 or (batch_idx+1) == len(loader):\n            \n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            # Update weights\n            scaler.step(optimizer)\n            scaler.update()\n\n            optimizer.zero_grad()\n        \n        with torch.inference_mode():\n            dice = dice_coefficient(outputs, masks)\n            iou = iou_score(outputs, masks)\n        \n        current_loss = loss.item() * accumulation_steps\n        running_loss += current_loss\n        running_dice += dice\n        running_iou += iou\n        num_batches += 1\n        \n        # Update progress bar\n        pbar.set_postfix({\n            'loss': f\"{running_loss/num_batches:.4f}\",\n            'dice': f\"{running_dice/num_batches:.4f}\",\n            'iou': f\"{running_iou/num_batches:.4f}\"\n        })\n        \n        # Clean up immediately\n        del images, masks, outputs, loss\n        \n        # Monitor memory every 50 batches\n        if batch_idx % 50 == 0:\n            mem_gb = monitor_memory_callback()\n            pbar.set_postfix({\n                'loss': f\"{running_loss/num_batches:.4f}\",\n                'dice': f\"{running_dice/num_batches:.4f}\",\n                'iou': f\"{running_iou/num_batches:.4f}\",\n                'mem': f\"{mem_gb:.1f}GB\"\n            })\n    \n    avg_loss = running_loss / num_batches\n    avg_dice = running_dice / num_batches\n    avg_iou = running_iou / num_batches\n    \n    return avg_loss, avg_dice, avg_iou\n\ndef validation_step_optimized(model, loader, loss_fn, device):\n    \"\"\"Validation with memory management\"\"\"\n    model.eval()\n    \n    running_loss = 0.0\n    running_dice = 0.0\n    running_iou = 0.0\n    num_batches = 0\n    \n    with torch.inference_mode():\n        pbar = tqdm(loader, desc=\"Validation\")\n        \n        for batch_idx, (images, masks) in enumerate(pbar):\n            images = images.to(device, non_blocking=True)\n            masks = masks.to(device, non_blocking=True)\n            \n            with autocast():\n                outputs = model(images)\n                loss = loss_fn(outputs, masks)\n            \n            dice = dice_coefficient(outputs, masks)\n            iou = iou_score(outputs, masks)\n            \n            running_loss += loss.detach().cpu().item()\n            running_dice += dice\n            running_iou += iou\n            num_batches += 1\n            \n            pbar.set_postfix({\n                'loss': f\"{running_loss/num_batches:.4f}\",\n                'dice': f\"{running_dice/num_batches:.4f}\",\n                'iou': f\"{running_iou/num_batches:.4f}\"\n            })\n            \n            # Clean up\n            del images, masks, outputs, loss\n            \n            # Aggressive memory clearing every 20 batches\n            if batch_idx % 20 == 0:\n                clear_memory()\n    \n    avg_loss = running_loss / num_batches\n    avg_dice = running_dice / num_batches\n    avg_iou = running_iou / num_batches\n    \n    return avg_loss, avg_dice, avg_iou\n\nprint(\"‚úÖ Memory-optimized training functions ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T13:07:33.961804Z","iopub.execute_input":"2025-11-22T13:07:33.962004Z","iopub.status.idle":"2025-11-22T13:07:33.978792Z","shell.execute_reply.started":"2025-11-22T13:07:33.961984Z","shell.execute_reply":"2025-11-22T13:07:33.978073Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.2. Login wandb & Configure wandb","metadata":{}},{"cell_type":"code","source":"!pip install -q wandb\n!wandb login <<wandb id>>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T13:07:33.979595Z","iopub.execute_input":"2025-11-22T13:07:33.980023Z","iopub.status.idle":"2025-11-22T13:07:38.615499Z","shell.execute_reply.started":"2025-11-22T13:07:33.980008Z","shell.execute_reply":"2025-11-22T13:07:38.614580Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CURRENT_DATE = datetime.now().strftime(\"%Y-%m-%d\")\nEXPERIMENT_NAME = f\"{CURRENT_DATE}_{ARCHITECTURE}_{ENCODER}_LR-{LR}_BS-{BATCH_SIZE}_EPOCHS-{EPOCHS}\"\nif EXTRA_NOTE:\n    EXPERIMENT_NAME += f\"_{EXTRA_NOTE}\"\n\n# Initialize wandb\nrun = wandb.init(\n    entity=WANDB_ENTITY,\n    project=WANDB_PROJECT,\n    name=EXPERIMENT_NAME,\n    config={\n        # Model\n        \"architecture\": ARCHITECTURE,\n        \"encoder\": ENCODER,\n        \"encoder_weights\": ENCODER_WEIGHTS,\n        \"in_channels\": N_SLICES,\n        \"decoder_attention\": \"scse\",\n        \n        # Data\n        \"image_size\": IMAGE_SIZE,\n        \"n_slices\": N_SLICES,\n        \"hu_window\": [HU_MIN, HU_MAX],\n        \"train_volumes\": len(train_volumes),\n        \"val_volumes\": len(val_volumes),\n        \"train_slices\": len(train_dataset),\n        \"val_slices\": len(val_dataset),\n        \n        # Training\n        \"batch_size\": BATCH_SIZE,\n        \"epochs\": EPOCHS,\n        \"learning_rate\": LR,\n        \"weight_decay\": WEIGHT_DECAY,\n        \"optimizer\": \"AdamW\",\n        \"scheduler\": \"CosineAnnealingLR\",\n        \"loss\": \"Combined (Dice+BCE)\",\n        \n        # Other\n        \"patience\": PATIENCE,\n        \"random_state\": RANDOM_STATE,\n    }\n)\n\nprint(f\"\\n{'='*70}\")\nprint(\"WANDB INITIALIZED\")\nprint(f\"{'='*70}\")\nprint(f\"‚úÖ Project: {WANDB_PROJECT}\")\nprint(f\"‚úÖ Experiment: {EXPERIMENT_NAME}\")\nprint(f\"{'='*70}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T13:07:38.616840Z","iopub.execute_input":"2025-11-22T13:07:38.617087Z","iopub.status.idle":"2025-11-22T13:07:49.381262Z","shell.execute_reply.started":"2025-11-22T13:07:38.617064Z","shell.execute_reply":"2025-11-22T13:07:49.380618Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.3. Training loop","metadata":{}},{"cell_type":"code","source":"print(f\"\\n{'='*70}\")\nprint(\"STARTING TRAINING (MEMORY OPTIMIZED)\")\nprint(f\"{'='*70}\")\n\n# Initialize\nscaler = GradScaler()\nbest_val_dice = -1\nbest_epoch = -1\nepochs_no_improve = 0\nearly_stop = False\naccumulation_steps = 4\n\nstart_time = time()\n\nfor epoch in range(EPOCHS):\n    print(f\"\\n{'='*70}\")\n    print(f\"Epoch: {epoch + 1}/{EPOCHS}\")\n    print(f\"{'='*70}\")\n    \n    # Training\n    train_loss, train_dice, train_iou = train_step_optimized(model=model, loader=train_loader, loss_fn=loss_fn, optimizer=optimizer, scaler=scaler, device=device, epoch=epoch, accumulation_steps=4)\n    clear_memory()\n    \n    # Validation\n    val_loss, val_dice, val_iou = validation_step_optimized(model=model, loader=val_loader, loss_fn=loss_fn, device=device)\n    clear_memory()\n    \n    # Update scheduler\n    scheduler.step()\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    # Print summary\n    print(f\"\\n{'‚îÄ'*70}\")\n    print(f\"EPOCH {epoch + 1} SUMMARY\")\n    print(f\"{'‚îÄ'*70}\")\n    print(f\"Train - Loss: {train_loss:.4f} | Dice: {train_dice:.4f}\")\n    print(f\"Val   - Loss: {val_loss:.4f} | Dice: {val_dice:.4f}\")\n    print(f\"Learning Rate: {current_lr:.7f}\")\n    print(f\"{'‚îÄ'*70}\")\n    \n    # Log to wandb\n    run.log({\n        \"epoch\": epoch + 1,\n        \"Loss/train\": train_loss,\n        \"Dice/train\": train_dice,\n        \"IoU/train\": train_iou,\n        \"Loss/val\": val_loss,\n        \"Dice/val\": val_dice,\n        \"IoU/val\": val_iou,\n        \"learning_rate\": current_lr,\n        \"memory_gb\": monitor_memory_callback()\n    })\n    \n    # Save best model\n    if val_dice > best_val_dice:\n        best_val_dice = val_dice\n        best_epoch = epoch + 1\n        epochs_no_improve = 0\n        \n        checkpoint = {\n            'epoch': epoch + 1,\n            'model_state_dict': model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'best_val_dice': best_val_dice,\n            'config': {\n                'architecture': ARCHITECTURE,\n                'encoder': ENCODER,\n                'image_size': IMAGE_SIZE,\n                'n_slices': N_SLICES,\n            }\n        }\n        \n        torch.save(checkpoint, \"best_model.pth\")\n        wandb.save(\"best_model.pth\", policy=\"now\")\n        \n        print(f\"‚úÖ New best model saved! Val Dice: {best_val_dice:.4f}\")\n        print(f\"   Early stopping: {epochs_no_improve}/{PATIENCE}\")\n    else:\n        epochs_no_improve += 1\n        print(f\"‚ö†Ô∏è  No improvement. Early stopping: {epochs_no_improve}/{PATIENCE}\")\n        \n        if epochs_no_improve >= PATIENCE:\n            print(f\"\\n‚ùå EARLY STOPPING at epoch {epoch + 1}\")\n            print(f\"Best: Epoch {best_epoch}, Dice {best_val_dice:.4f}\")\n            early_stop = True\n    \n    if early_stop:\n        break\n    \n    # Final cleanup for epoch\n    clear_memory()\n\n# Training completed\ntotal_training_time = (time() - start_time) / 60\n\nprint(f\"\\n{'='*70}\")\nprint(\"TRAINING COMPLETED\")\nprint(f\"{'='*70}\")\nprint(f\"‚úÖ Best Val Dice: {best_val_dice:.4f}\")\nprint(f\"‚úÖ Best Epoch: {best_epoch}/{epoch + 1}\")\nprint(f\"‚úÖ Total Training Time: {total_training_time:.2f} minutes\")\nprint(f\"{'='*70}\")\n\n# Update wandb summary\nrun.summary[\"best_val_dice\"] = best_val_dice\nrun.summary[\"best_val_epoch\"] = best_epoch\nrun.summary[\"total_training_time_minutes\"] = total_training_time\n\n# Finish wandb\nwandb.finish()\n\n# Final cleanup\nclear_memory()\n\nprint(\"\\n‚úÖ All done! Check your wandb dashboard for detailed results.\")\nprint(f\"‚úÖ Best model saved as: best_model.pth\")","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-11-22T13:07:49.382251Z","iopub.execute_input":"2025-11-22T13:07:49.382788Z","execution_failed":"2025-11-22T13:09:56.417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Inference","metadata":{}},{"cell_type":"markdown","source":"## 5.1. Pre-processing test-set (1 time only)","metadata":{}},{"cell_type":"code","source":"# def pre_extract_slices_test(volume_list, output_dir, n_slices=3):\n#     \"\"\"\n#     Pre-extract slices for TEST set (NO LABELS)\n#     Output: preprocessed 2.5D images saved to disk + metadata_test.csv\n    \n#     Args:\n#         volume_list: list of {\"image\": path}\n#         output_dir: folder to save\n#         n_slices: 2.5D slices count\n#     \"\"\"\n#     os.makedirs(output_dir, exist_ok=True)\n#     os.makedirs(os.path.join(output_dir, \"images\"), exist_ok=True)\n\n#     half_slices = n_slices // 2\n#     slice_metadata = []\n\n#     print(f\"\\n{'='*70}\")\n#     print(\"PREPROCESSING TEST SET\")\n#     print(f\"{'='*70}\")\n\n#     for vol_idx, vol_dict in enumerate(tqdm(volume_list, desc=\"Test Extracting\")):\n\n#         # ---- Load CT ----\n#         img_nii = nib.load(vol_dict['image'])\n#         img_data = img_nii.get_fdata().astype(np.float32)\n\n#         # ---- Apply same HU normalization as train/val ----\n#         img_data = np.clip(img_data, HU_MIN, HU_MAX)\n#         img_data = (img_data - HU_MIN) / (HU_MAX - HU_MIN)\n\n#         H, W, Z = img_data.shape\n\n#         for z_idx in range(Z):\n\n#             # ---- Build 2.5D slices (e.g. 3 slices: z-1, z, z+1) ----\n#             slices_list = []\n#             for offset in range(-half_slices, half_slices + 1):\n#                 z = np.clip(z_idx + offset, 0, Z - 1)\n#                 slices_list.append(img_data[:, :, z])\n\n#             image_25d = np.stack(slices_list, axis=-1)   # (H, W, 3)\n\n#             # ---- Resize (same as train/val) ----\n#             image_25d = cv2.resize(\n#                 image_25d,\n#                 (IMAGE_SIZE, IMAGE_SIZE),\n#                 interpolation=cv2.INTER_LINEAR\n#             )\n\n#             # ---- Save ----\n#             slice_id = f\"vol{vol_idx:03d}_slice{z_idx:03d}\"\n#             img_path = os.path.join(output_dir, \"images\", f\"{slice_id}.npz\")\n#             np.savez_compressed(img_path, data=image_25d)\n\n#             slice_metadata.append({\n#                 'image_path': img_path,\n#                 'volume_id': vol_idx,\n#                 'slice_id': slice_id,\n#                 'orig_H': H,\n#                 'orig_W': W\n#                 'z_idx': z_idx,\n#             })\n\n#         del img_data\n#         gc.collect()\n\n#     # ---- Save metadata ----\n#     metadata_path = os.path.join(output_dir, \"metadata_test.csv\")\n#     pd.DataFrame(slice_metadata).to_csv(metadata_path, index=False)\n\n#     print(f\"‚úÖ Extracted {len(slice_metadata)} test slices\")\n#     print(f\"üìÑ Metadata saved at: {metadata_path}\")\n\n#     return metadata_path\n\n# test_image_paths = sorted(glob.glob(\"/kaggle/input/aio2025liverseg/test/*.nii\"))\n\n# test_volumes = [\n#     {\"image\": img_path}\n#     for img_path in test_image_paths\n# ]\n\n# metadata_test_csv = pre_extract_slices_test(\n#     volume_list=test_volumes,\n#     output_dir=\"./preprocessed_data/test\",\n#     n_slices=N_SLICES\n# )","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-22T13:09:56.417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.2. Load Data + Model","metadata":{}},{"cell_type":"markdown","source":"### Load Dataset","metadata":{}},{"cell_type":"code","source":"def keep_largest_component_3d(mask_3d):\n    \"\"\"Keep only largest connected component\"\"\"\n    if mask_3d.sum() == 0:\n        return mask_3d\n    \n    labeled = label(mask_3d, connectivity=3)\n    if labeled.max() == 0:\n        return mask_3d\n    \n    component_sizes = np.bincount(labeled.ravel())\n    component_sizes[0] = 0\n    largest_id = component_sizes.argmax()\n    \n    return (labeled == largest_id).astype(np.uint8)\n\ndef fill_holes_3d(mask_3d):\n    \"\"\"Fill holes slice-by-slice\"\"\"\n    result = mask_3d.copy()\n    for z in range(mask_3d.shape[2]):\n        result[:, :, z] = binary_fill_holes(mask_3d[:, :, z]).astype(np.uint8)\n    return result\n\ndef postprocess_volume(pred_volume, min_size=2000):\n    \"\"\"Complete post-processing pipeline\"\"\"\n    # Remove small objects\n    mask = remove_small_objects(\n        pred_volume.astype(bool),\n        min_size=min_size,\n        connectivity=3\n    ).astype(np.uint8)\n    \n    # Keep largest component\n    mask = keep_largest_component_3d(mask)\n    \n    # Fill holes\n    mask = fill_holes_3d(mask)\n    \n    return mask\n    \nclass PreExtractedTestDataset(Dataset):\n    def __init__(self, metadata_csv, transform=None):\n        df = pd.read_csv(metadata_csv)\n        df[\"image_path\"] = df[\"image_path\"].apply(lambda x: os.path.abspath(x))\n        self.metadata = df            \n        self.transform = transform\n        self.base_dir = os.path.dirname(metadata_csv)\n\n    def __len__(self):\n        return len(self.metadata)\n    \n    def __getitem__(self, idx):\n        row = self.metadata.iloc[idx]\n        img_name = os.path.basename(row['image_path'])\n        image_path = os.path.join(self.base_dir, \"images\", img_name)\n\n        image = np.load(image_path)['data'].astype(np.float32)\n\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n\n        return {\n            \"image\": image,\n            \"slice_id\": row[\"slice_id\"],\n            \"volume_id\": row[\"volume_id\"],\n            \"z_idx\": row[\"z_idx\"]\n        }\n\ndef rle_encode(mask: np.ndarray) -> str:\n    \"\"\"\n    Run-length encode a 2D binary mask (1 for foreground, 0 for background)\n    Empty mask -> '1 0'.\n    \"\"\"\n    assert mask.ndim == 2, \"rle_encode expects a 2D mask\"\n    pixels = mask.astype(np.uint8).flatten(order='F')  # column-major\n    if pixels.max() == 0:\n        return \"1 0\"\n    # Pad with zeros at both ends to catch transitions cleanly\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return \" \".join(map(str, runs))\n\ndef test_transforms():\n    \"\"\"Testing transform (no augmentation)\"\"\"\n    return A.Compose([\n        # A.Resize(height=256, width=256, interpolation=cv2.INTER_LINEAR, mask_interpolation=cv2.INTER_NEAREST), # Only resizing if we're training on 256x256 images\n        ToTensorV2()])\n\nprint(\"‚úÖ Post-processing functions loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T13:47:43.202778Z","iopub.execute_input":"2025-11-23T13:47:43.203167Z","iopub.status.idle":"2025-11-23T13:47:43.326979Z","shell.execute_reply.started":"2025-11-23T13:47:43.203140Z","shell.execute_reply":"2025-11-23T13:47:43.326087Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Post-processing functions loaded\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"print(f\"\\n{'='*70}\")\nprint(\"LOADING BEST MODEL\")\nprint(f\"{'='*70}\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n### Load checkpoint\ncheckpoint = torch.load(\"/kaggle/input/unetplusplus-512x512/pytorch/default/2/2d_unetplusplust_epoch29_size512\", map_location=\"cpu\")\n# checkpoint = torch.load(\"best_model.pth\", map_location=\"cpu\")\n\n# Recreate model\ninference_model = smp.UnetPlusPlus(\n    encoder_name=ENCODER,\n    encoder_weights=None,  # No pretrained weights needed\n    in_channels=N_SLICES,\n    classes=1,\n    activation=None,\n    decoder_attention_type=\"scse\"\n)\n\n# Load weights\ninference_model.load_state_dict(checkpoint['model_state_dict'])\ninference_model = inference_model.to(device)\ninference_model.eval()\n\nprint(f\"‚úÖ Model loaded successfully!\")\nprint(f\"‚úÖ Best Val Dice from training: {checkpoint['best_val_dice']:.4f}\")\nprint(f\"‚úÖ Trained at epoch: {checkpoint['epoch']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T13:47:53.191325Z","iopub.execute_input":"2025-11-23T13:47:53.192332Z","iopub.status.idle":"2025-11-23T13:47:57.377473Z","shell.execute_reply.started":"2025-11-23T13:47:53.192288Z","shell.execute_reply":"2025-11-23T13:47:57.376580Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nLOADING BEST MODEL\n======================================================================\n‚úÖ Model loaded successfully!\n‚úÖ Best Val Dice from training: 0.9584\n‚úÖ Trained at epoch: 21\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 5.3 Inference loop","metadata":{}},{"cell_type":"code","source":"# ---- Load metadata ----\nmetadata_csv=\"/kaggle/input/preprocessed-data/preprocessed_data/test/metadata_test.csv\" \nTEST_DIR = \"/kaggle/input/aio2025liverseg/test\"\ntest_volume_paths = sorted(glob.glob(f\"{TEST_DIR}/*.nii\"))\ndf = pd.read_csv(metadata_csv)\n\n# ---- Load original volume shapes ----\noriginal_shapes = {}\nfor vol_id, grp in pd.read_csv(metadata_csv).groupby(\"volume_id\"):\n    H = int(grp[\"orig_H\"].iloc[0])\n    W = int(grp[\"orig_W\"].iloc[0])\n    Z = int(grp[\"z_idx\"].max()) + 1\n    original_shapes[int(vol_id)] = (H, W, Z)\n\n# ---- Dataset & Loader ----\ntest_ds = PreExtractedTestDataset(metadata_csv, transform=test_transforms())\ntest_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=NUM_WORKERS)\n\n# ---- Storage ----\npred_masks = {}   # pred_masks[vol_id][z] = 2D mask original size\n\nfor vol_id in range(len(test_volume_paths)):\n    pred_masks[vol_id] = {}\n\n# ---- Inference ----\nprint(\"\\n\" + \"=\"*70)\nprint(\"RUNNING INFERENCE\")\nprint(\"=\"*70)\n\nwith torch.inference_mode():\n    for batch in tqdm(test_loader, desc=\"Predicting\"):\n        imgs = batch[\"image\"].to(device)       # (B,3,H,W)\n\n        logits = inference_model(imgs)                   # -> (B,1,H,W)\n        probs = torch.sigmoid(logits)\n        preds = (probs > 0.5).float().cpu().numpy()\n\n        # Loop t·ª´ng slice\n        for i in range(len(preds)):\n            pred_resized = preds[i,0,:,:]  # (IMAGE_SIZE, IMAGE_SIZE)\n\n            vol_id = int(batch[\"volume_id\"][i])\n            z = int(batch[\"z_idx\"][i])\n\n            orig_H, orig_W, _ = original_shapes[vol_id]\n\n            # ---- Resize back to original size ----\n            mask_original = cv2.resize(\n                pred_resized.astype(np.uint8),\n                (orig_W, orig_H),\n                interpolation=cv2.INTER_NEAREST\n            )\n\n            pred_masks[vol_id][z] = mask_original\n\n# ---- POST-PROCESSING----\nprint(\"\\n\" + \"=\"*70)\nprint(\"APPLYING POST-PROCESSING\")\nprint(\"=\"*70)\n\nfor vol_id in tqdm(range(len(test_volume_paths)), desc=\"Post-processing\"):\n    H, W, Z = original_shapes[vol_id]\n    \n    # Stack slices into 3D volume\n    pred_volume = np.zeros((H, W, Z), dtype=np.uint8)\n    for z in range(Z):\n        pred_volume[:, :, z] = pred_masks[vol_id][z]\n    \n    # Apply post-processing\n    pred_volume_cleaned = postprocess_volume(pred_volume, min_size=2000)\n    \n    # Update pred_masks with cleaned predictions\n    for z in range(Z):\n        pred_masks[vol_id][z] = pred_volume_cleaned[:, :, z]\n\nprint(\"‚úÖ Post-processing completed!\")\n\n# ---- Create CSV ----\nprint(\"\\n\" + \"=\"*70)\nprint(\"CREATING SUBMISSION\")\nprint(\"=\"*70)\n\nsubmission = []\nfor vol_id in range(len(test_volume_paths)):\n    Z = original_shapes[vol_id][2]\n\n    for z in range(Z):\n        mask = pred_masks[vol_id][z]\n        rle = rle_encode(mask)\n        id_str = f\"cell_{vol_id+80}_{z}\"\n        submission.append([id_str, rle])\n\n# ---- Save CSV ----\ndf_sub = pd.DataFrame(submission, columns=[\"id\", \"rle\"])\ndf_sub.to_csv(\"submission.csv\", index=False)\n\nprint(\"‚úÖ submission.csv created!\")\nprint(f\"‚úÖ Total predictions: {len(submission)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T11:16:04.711014Z","iopub.execute_input":"2025-11-23T11:16:04.711293Z","iopub.status.idle":"2025-11-23T11:21:38.887438Z","shell.execute_reply.started":"2025-11-23T11:16:04.711272Z","shell.execute_reply":"2025-11-23T11:21:38.886301Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Visualization","metadata":{}},{"cell_type":"markdown","source":"## Visualize random prediction on Val set","metadata":{}},{"cell_type":"code","source":"import random\nimport matplotlib.pyplot as plt\n\n# random.seed(42)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = inference_model\nmodel = model.to(device)\nmodel.eval()\n\nVAL_DIR = \"/kaggle/input/preprocess-2d-data/preprocessed_data/val\"\nval_image_paths = sorted(glob.glob(f\"{VAL_DIR}/images/*.npz\"))\nval_mask_paths = sorted(glob.glob(f\"{VAL_DIR}/masks/*.npz\"))\ntotal_image = len(val_image_paths)\n\nrandom_indices = random.sample(range(total_image), k=10)\n\nfig = plt.figure(figsize=(12, 4 * len(random_indices)))\n\nfor index, random_index in enumerate(random_indices):\n    image = np.load(val_image_paths[random_index])[\"data\"].astype(np.float32)\n    mask = np.load(val_mask_paths[random_index])[\"data\"].astype(np.float32)\n\n    original_image = image[:, :, 1]\n\n    # Convert image to tensor\n    image_tensor = ToTensorV2()(image=image)[\"image\"] # (1, H, W)\n    image_tensor = image_tensor.unsqueeze(0).to(device)\n    \n    with torch.inference_mode():\n        logits = model(image_tensor)\n        pred_image = (torch.sigmoid(logits).cpu().numpy() > 0.5).astype(np.uint8).squeeze()\n\n    # Visualize\n    ax1 = plt.subplot(len(random_indices), 3, index*3 + 1)\n    ax1.imshow(original_image, cmap=\"gray\")\n    ax1.imshow(mask, cmap=\"Blues\", alpha=0.5)\n    ax1.set_title(f\"Mask/Image, image: {random_index}\", fontsize=13)\n    ax1.axis(\"off\")\n\n    ax2 = plt.subplot(len(random_indices), 3, index*3 + 2)\n    ax2.imshow(original_image, cmap=\"gray\")\n    ax2.imshow(pred_image, cmap=\"Reds\", alpha=0.5)\n    ax2.set_title(\"Prediction/Image\", fontsize=13)\n    ax2.axis(\"off\")\n\n    ax3 = plt.subplot(len(random_indices), 3, index*3 + 3)\n    fp = np.logical_and(pred_image == 1, mask == 0)   # False Positive\n    fn = np.logical_and(pred_image == 0, mask == 1)   # False Negative\n    ax3.imshow(fp, cmap=\"Blues\", alpha=fp * 0.8)\n    ax3.imshow(fn, cmap=\"Reds\", alpha=fn * 0.8)\n    ax3.set_title(\"Error Map (FP=Red, FN=Blue)\", fontsize=13)\n    ax3.axis(\"off\")\n    \nplt.tight_layout()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T07:08:48.871014Z","iopub.execute_input":"2025-11-23T07:08:48.871344Z","iopub.status.idle":"2025-11-23T07:08:55.572848Z","shell.execute_reply.started":"2025-11-23T07:08:48.871304Z","shell.execute_reply":"2025-11-23T07:08:55.571569Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize k random slices by 4 bins (IQR)","metadata":{}},{"cell_type":"code","source":"import os, glob, random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom albumentations.pytorch import ToTensorV2\n\n# ==================== CONFIG =====================\nVAL_DIR = \"/kaggle/input/preprocess-2d-data/preprocessed_data/val\"\nK = 5   # s·ªë ·∫£nh mu·ªën xem cho m·ªói bin\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = inference_model.to(DEVICE).eval()\n# =================================================\n\n\ndef dice_score(mask, pred):\n    mask = mask.astype(bool)\n    pred = pred.astype(bool)\n\n    if mask.sum() == 0 and pred.sum() == 0:\n        return 1.0\n    if mask.sum() == 0 and pred.sum() > 0:\n        return 0.0\n\n    inter = (mask & pred).sum()\n    dice = 2 * inter / (mask.sum() + pred.sum())\n    return float(dice)\n\n\n# ===== Read val slices =====\nval_images = sorted(glob.glob(f\"{VAL_DIR}/images/*.npz\"))\nval_masks = sorted(glob.glob(f\"{VAL_DIR}/masks/*.npz\"))\n\nN = len(val_images)\nforeground_ratio = []\n\nfor i in range(N):\n    m = np.load(val_masks[i])[\"data\"].astype(np.float32)\n    fg = (m > 0).mean()\n    foreground_ratio.append(fg)\n\nforeground_ratio = np.array(foreground_ratio)\n\n# T√≠nh 4 bin theo quartile\nq1, q2, q3 = np.percentile(foreground_ratio, [25, 50, 75])\n\nbins = [\n    np.where(foreground_ratio <= q1)[0],                        # Bin 1\n    np.where((foreground_ratio > q1) & (foreground_ratio <= q2))[0],  # Bin 2\n    np.where((foreground_ratio > q2) & (foreground_ratio <= q3))[0],  # Bin 3\n    np.where(foreground_ratio > q3)[0],                         # Bin 4\n]\n\nbin_names = [\"Low FG\", \"Medium-Low\", \"Medium-High\", \"High FG\"]\n\n# ==================== Visualization =====================\nfor b_idx, indices in enumerate(bins):\n    if len(indices) == 0:\n        continue\n\n    print(f\"\\n=== Bin {b_idx+1} ({bin_names[b_idx]}) ‚Äî showing {K} random slices ===\")\n\n    chosen = random.sample(list(indices), min(K, len(indices)))\n\n    fig = plt.figure(figsize=(12, 4 * len(chosen)))\n\n    for j, idx in enumerate(chosen):\n\n        img = np.load(val_images[idx])[\"data\"].astype(np.float32)\n        mask = np.load(val_masks[idx])[\"data\"].astype(np.float32)\n        original = img[:, :, 1]  # slice middle n·∫øu l√† 3-channel\n\n        # tensor\n        tensor = ToTensorV2()(image=img)[\"image\"].unsqueeze(0).to(DEVICE)\n\n        with torch.inference_mode():\n            logit = model(tensor)\n            pred = (torch.sigmoid(logit).cpu().numpy() > 0.5).astype(np.uint8).squeeze()\n\n        dice = dice_score(mask, pred)\n\n        # ==== Plot layout: mask, pred, error ====\n        ax1 = plt.subplot(len(chosen), 3, j*3 + 1)\n        ax1.imshow(original, cmap=\"gray\")\n        ax1.imshow(mask, cmap=\"Blues\", alpha=0.5)\n        ax1.set_title(f\"Mask/Image (idx={idx})\")\n        ax1.axis(\"off\")\n\n        ax2 = plt.subplot(len(chosen), 3, j*3 + 2)\n        ax2.imshow(original, cmap=\"gray\")\n        ax2.imshow(pred, cmap=\"Reds\", alpha=0.5)\n        ax2.set_title(f\"Prediction/Image\\nDice={dice:.4f}\")\n        ax2.axis(\"off\")\n\n        # Error map\n        fp = (pred == 1) & (mask == 0)\n        fn = (pred == 0) & (mask == 1)\n\n        ax3 = plt.subplot(len(chosen), 3, j*3 + 3)\n        ax3.imshow(fp, cmap=\"Reds\", alpha=fp*0.8)\n        ax3.imshow(fn, cmap=\"Blues\", alpha=fn*0.8)\n        ax3.set_title(\"Error Map\")\n        ax3.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T07:33:58.335353Z","iopub.execute_input":"2025-11-23T07:33:58.335738Z","iopub.status.idle":"2025-11-23T07:34:20.398044Z","shell.execute_reply.started":"2025-11-23T07:33:58.335713Z","shell.execute_reply":"2025-11-23T07:34:20.396817Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize top K bad predictions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom albumentations.pytorch import ToTensorV2\nimport glob\nfrom tdqm.auto import tdqm\n\ndef dice_score(mask, pred):\n    mask = mask.astype(bool)\n    pred = pred.astype(bool)\n\n    if mask.sum() == 0 and pred.sum() == 0:\n        return 1.0\n    if mask.sum() == 0 and pred.sum() > 0:\n        return 0.0\n\n    inter = (mask & pred).sum()\n    return 2 * inter / (mask.sum() + pred.sum())\n\n\n# ===== Read val slices =====\nval_images = sorted(glob.glob(f\"{VAL_DIR}/images/*.npz\"))\nval_masks = sorted(glob.glob(f\"{VAL_DIR}/masks/*.npz\"))\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nscores = []\n\n# Compute Dice for entire valset\nfor idx in tdqm(range(len(val_images)), desc=\"Inference in Validation data\"):\n    img = np.load(val_images[idx])[\"data\"].astype(np.float32)\n    mask = np.load(val_masks[idx])[\"data\"].astype(np.float32)\n\n    tensor = ToTensorV2()(image=img)[\"image\"].unsqueeze(0).to(DEVICE)\n\n    with torch.inference_mode():\n        logit = model(tensor)\n        pred = (torch.sigmoid(logit).cpu().numpy() > 0.5).astype(np.uint8).squeeze()\n\n    dice = dice_score(mask, pred)\n    scores.append((dice, idx))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T07:24:48.856655Z","iopub.execute_input":"2025-11-23T07:24:48.856997Z","iopub.status.idle":"2025-11-23T07:31:44.114950Z","shell.execute_reply.started":"2025-11-23T07:24:48.856976Z","shell.execute_reply":"2025-11-23T07:31:44.113872Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================== CONFIG ======================\nK = 30  # The number of bad predictions that we want to visualize\n# =================================================\n\n# Sort by Dice ascending (worst first)\nscores = sorted(scores, key=lambda x: x[0])\n\n# Take top K worst\nworst = scores[:K]\n\nprint(\"\\n=== Worst K slices by Dice ===\")\nfor d, idx in worst:\n    print(f\"idx={idx}, Dice={d:.4f}\")\n# ================= Visualization ==================\nfig = plt.figure(figsize=(12, 4 * K))\n\nfor j, (d, idx) in enumerate(worst):\n    img = np.load(val_images[idx])[\"data\"].astype(np.float32)\n    mask = np.load(val_masks[idx])[\"data\"].astype(np.float32)\n    original = img[:, :, 1]\n\n    tensor = ToTensorV2()(image=img)[\"image\"].unsqueeze(0).to(DEVICE)\n\n    with torch.inference_mode():\n        logit = model(tensor)\n        pred = (torch.sigmoid(logit).cpu().numpy() > 0.5).astype(np.uint8).squeeze()\n\n    # Plot\n    ax1 = plt.subplot(K, 3, j*3 + 1)\n    ax1.imshow(original, cmap=\"gray\")\n    ax1.imshow(mask, cmap=\"Blues\", alpha=0.5)\n    ax1.set_title(f\"Mask/Image (idx={idx})\")\n    ax1.axis(\"off\")\n\n    ax2 = plt.subplot(K, 3, j*3 + 2)\n    ax2.imshow(original, cmap=\"gray\")\n    ax2.imshow(pred, cmap=\"Reds\", alpha=0.5)\n    ax2.set_title(f\"Prediction/Image\\nDice={d:.4f}\")\n    ax2.axis(\"off\")\n\n    fp = (pred == 1) & (mask == 0)\n    fn = (pred == 0) & (mask == 1)\n\n    ax3 = plt.subplot(K, 3, j*3 + 3)\n    ax3.imshow(fp, cmap=\"Reds\", alpha=fp*0.8)\n    ax3.imshow(fn, cmap=\"Blues\", alpha=fn*0.8)\n    ax3.set_title(\"Error Map\")\n    ax3.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T07:36:06.620670Z","iopub.execute_input":"2025-11-23T07:36:06.621422Z","iopub.status.idle":"2025-11-23T07:36:21.543802Z","shell.execute_reply.started":"2025-11-23T07:36:06.621395Z","shell.execute_reply":"2025-11-23T07:36:21.542902Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null}]}