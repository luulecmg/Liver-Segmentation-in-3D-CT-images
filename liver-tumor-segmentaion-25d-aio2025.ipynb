{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Overview pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "# ‚îÇ  CT Volume (512, 512, Z)             ‚îÇ\n",
    "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "#                ‚îÇ\n",
    "#                ‚îÇ Loop over each slice\n",
    "#                ‚ñº\n",
    "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "# ‚îÇ  Slice 2D ‚Üí Resize ‚Üí Predict         ‚îÇ\n",
    "# ‚îÇ  Repeat Z times                      ‚îÇ\n",
    "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "#                ‚îÇ\n",
    "#                ‚îÇ Stack predictions\n",
    "#                ‚ñº\n",
    "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "# ‚îÇ  Prediction Volume (512, 512, Z)     ‚îÇ ‚Üê 3D VOLUME\n",
    "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "#                ‚îÇ\n",
    "#                ‚îÇ POST-PROCESSING 3D:\n",
    "#                ‚îÇ 1. Remove small objects\n",
    "#                ‚îÇ 2. Keep largest component\n",
    "#                ‚îÇ 3. Fill holes\n",
    "#                ‚ñº\n",
    "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "# ‚îÇ  Cleaned Volume (512, 512, Z)        ‚îÇ\n",
    "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "#                ‚îÇ\n",
    "#                ‚îÇ Loop over each slice\n",
    "#                ‚ñº\n",
    "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "# ‚îÇ  RLE encode each slice               ‚îÇ\n",
    "# ‚îÇ  ‚Üí submission.csv                    ‚îÇ\n",
    "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries & Configre parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:42:56.619497Z",
     "iopub.status.busy": "2025-11-23T13:42:56.618864Z",
     "iopub.status.idle": "2025-11-23T13:44:11.793124Z",
     "shell.execute_reply": "2025-11-23T13:44:11.792286Z",
     "shell.execute_reply.started": "2025-11-23T13:42:56.619459Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-23T13:44:11.794965Z",
     "iopub.status.busy": "2025-11-23T13:44:11.794673Z",
     "iopub.status.idle": "2025-11-23T13:44:24.327563Z",
     "shell.execute_reply": "2025-11-23T13:44:24.326763Z",
     "shell.execute_reply.started": "2025-11-23T13:44:11.794936Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Import libraries completed!\n",
      "‚úÖ PyTorch version: 2.6.0+cu124\n",
      "‚úÖ CUDA available: True\n",
      "‚úÖ GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.morphology import remove_small_objects\n",
    "from skimage.measure import label\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"‚úÖ Import libraries completed!\")\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Config params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:44:24.328831Z",
     "iopub.status.busy": "2025-11-23T13:44:24.328407Z",
     "iopub.status.idle": "2025-11-23T13:44:24.337265Z",
     "shell.execute_reply": "2025-11-23T13:44:24.336461Z",
     "shell.execute_reply.started": "2025-11-23T13:44:24.328810Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HYPERPARAMETERS\n",
      "======================================================================\n",
      "Image size: 512x512\n",
      "2.5D approach: 3 consecutive slices\n",
      "HU window: [-100, 250]\n",
      "Batch size: 32\n",
      "Learning rate: 0.0001\n",
      "Max epochs: 100\n",
      "Model: UnetPlusPlus\n",
      "Encoder: efficientnet-b5\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "HU_MIN, HU_MAX = -100, 250  # Hounsfield Units window for liver\n",
    "IMAGE_SIZE = 512  # Resize slices to 512x512\n",
    "N_SLICES = 3  # 2.5D: use 3 consecutive slices as channels\n",
    "\n",
    "### Data paths\n",
    "IMAGE_DIR = \"/kaggle/input/aio2025liverseg/train/volume\"\n",
    "LABEL_DIR = \"/kaggle/input/aio2025liverseg/train/segmentation\"\n",
    "TEST_DIR = \"/kaggle/input/aio2025liverseg/test\"\n",
    "\n",
    "### Data split\n",
    "TRAIN_SIZE = 0.875  # 87.5% train, 12.5% val\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "### Training hyperparameters\n",
    "BATCH_SIZE = 32  \n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 100\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "### Early stopping\n",
    "PATIENCE = 15\n",
    "\n",
    "### Model architecture\n",
    "ARCHITECTURE = 'UnetPlusPlus'\n",
    "ENCODER = 'efficientnet-b5'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "### Wandb config\n",
    "WANDB_ENTITY = \"levanluucmg-aio\"\n",
    "WANDB_PROJECT = \"liver_tumor_segmentation_in_3D_images_kaggle\"\n",
    "EXTRA_NOTE = \"2.5D_UNet++_EfficientNetB5_NOaugment\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"2.5D approach: {N_SLICES} consecutive slices\")\n",
    "print(f\"HU window: [{HU_MIN}, {HU_MAX}]\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LR}\")\n",
    "print(f\"Max epochs: {EPOCHS}\")\n",
    "print(f\"Model: {ARCHITECTURE}\")\n",
    "print(f\"Encoder: {ENCODER}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Preprocessing (offline - run 1 time only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:27:20.328627Z",
     "iopub.status.busy": "2025-11-23T00:27:20.328447Z",
     "iopub.status.idle": "2025-11-23T00:27:22.555985Z",
     "shell.execute_reply": "2025-11-23T00:27:22.555373Z",
     "shell.execute_reply.started": "2025-11-23T00:27:20.328612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def pre_extract_slices_to_disk(volume_list, output_dir, n_slices=3, min_liver_pixels=100):\n",
    "#     \"\"\"\n",
    "#     Pre-extract all valid slices and save to disk\n",
    "#     Run this ONCE before training\n",
    "    \n",
    "#     Args:\n",
    "#         volume_list: List of volume dicts\n",
    "#         output_dir: Directory to save extracted slices\n",
    "#         n_slices: Number of slices for 2.5D\n",
    "#         min_liver_pixels: Minimum liver pixels\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     os.makedirs(os.path.join(output_dir, \"images\"), exist_ok=True)\n",
    "#     os.makedirs(os.path.join(output_dir, \"masks\"), exist_ok=True)\n",
    "    \n",
    "#     half_slices = n_slices // 2\n",
    "#     slice_metadata = []\n",
    "    \n",
    "#     print(f\"{'='*70}\")\n",
    "#     print(f\"PRE-EXTRACTING SLICES TO DISK: {output_dir}\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     for vol_idx, vol_dict in enumerate(tqdm(volume_list, desc=\"Extracting\")):\n",
    "#         # Load volume\n",
    "#         img_nii = nib.load(vol_dict['image'])\n",
    "#         img_data = img_nii.get_fdata().astype(np.float32)\n",
    "        \n",
    "#         label_nii = nib.load(vol_dict['label'])\n",
    "#         label_data = label_nii.get_fdata().astype(np.uint8)\n",
    "        \n",
    "#         # Preprocess\n",
    "#         img_data = np.clip(img_data, HU_MIN, HU_MAX)\n",
    "#         img_data = (img_data - HU_MIN) / (HU_MAX - HU_MIN)\n",
    "#         label_data = (label_data > 0).astype(np.uint8)\n",
    "        \n",
    "#         # Extract slices\n",
    "#         n_slices_total = img_data.shape[2]\n",
    "        \n",
    "#         for z_idx in range(n_slices_total):\n",
    "#             # Filter\n",
    "#             mask_slice = label_data[:, :, z_idx]\n",
    "#             if (mask_slice > 0).sum() < min_liver_pixels:\n",
    "#                 continue\n",
    "            \n",
    "#             # Extract 2.5D\n",
    "#             slices_list = []\n",
    "#             for offset in range(-half_slices, half_slices + 1):\n",
    "#                 z = z_idx + offset\n",
    "#                 z = max(0, min(z, n_slices_total - 1))\n",
    "#                 slices_list.append(img_data[:, :, z])\n",
    "            \n",
    "#             image_25d = np.stack(slices_list, axis=-1)  # (H, W, 3)\n",
    "#             mask_2d = mask_slice\n",
    "            \n",
    "#             # Resize\n",
    "#             image_25d = cv2.resize(image_25d, (IMAGE_SIZE, IMAGE_SIZE), \n",
    "#                                   interpolation=cv2.INTER_LINEAR)\n",
    "#             mask_2d = cv2.resize(mask_2d, (IMAGE_SIZE, IMAGE_SIZE), \n",
    "#                                 interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "#             # Save as NPZ (compressed)\n",
    "#             slice_id = f\"vol{vol_idx:03d}_slice{z_idx:03d}\"\n",
    "            \n",
    "#             img_path = os.path.join(output_dir, \"images\", f\"{slice_id}.npz\")\n",
    "#             mask_path = os.path.join(output_dir, \"masks\", f\"{slice_id}.npz\")\n",
    "            \n",
    "#             np.savez_compressed(img_path, data=image_25d)\n",
    "#             np.savez_compressed(mask_path, data=mask_2d)\n",
    "            \n",
    "#             slice_metadata.append({\n",
    "#                 'image_path': img_path,\n",
    "#                 'mask_path': mask_path,\n",
    "#                 'slice_id': slice_id\n",
    "#             })\n",
    "        \n",
    "#         del img_data, label_data\n",
    "#         gc.collect()\n",
    "    \n",
    "#     # Save metadata\n",
    "#     metadata_df = pd.DataFrame(slice_metadata)\n",
    "#     metadata_df.to_csv(os.path.join(output_dir, \"metadata.csv\"), index=False)\n",
    "    \n",
    "#     print(f\"‚úÖ Extracted {len(slice_metadata)} slices\")\n",
    "#     print(f\"‚úÖ Saved to: {output_dir}\")\n",
    "    \n",
    "#     return metadata_df\n",
    "\n",
    "\n",
    "# image_file_paths = sorted(glob.glob(f\"{IMAGE_DIR}/*.nii\"))\n",
    "# label_file_paths = sorted(glob.glob(f\"{LABEL_DIR}/*.nii\"))\n",
    "\n",
    "# # Create volume list\n",
    "# volume_files = [\n",
    "#     {\"image\": img_path, \"label\": lbl_path}\n",
    "#     for img_path, lbl_path in zip(image_file_paths, label_file_paths)\n",
    "# ]\n",
    "\n",
    "# print(f\"\\n{'='*70}\")\n",
    "# print(\"DATA LOADING\")\n",
    "# print(f\"{'='*70}\")\n",
    "# print(f\"‚úÖ Found {len(volume_files)} 3D CT volumes\")\n",
    "\n",
    "# ### Train/Val split (IMPORTANT: split by volumes, not slices!)\n",
    "# train_volumes, val_volumes = train_test_split(\n",
    "#     volume_files,\n",
    "#     train_size=TRAIN_SIZE,\n",
    "#     random_state=RANDOM_STATE,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# print(f\"üìä Train volumes: {len(train_volumes)}\")\n",
    "# print(f\"üìä Val volumes: {len(val_volumes)}\")\n",
    "\n",
    "# # Run pre-extraction ONCE\n",
    "# print(\"\\nüîß Running pre-extraction...\")\n",
    "# train_metadata = pre_extract_slices_to_disk(\n",
    "#     volume_list=train_volumes,\n",
    "#     output_dir=\"./preprocessed_data/train\",\n",
    "#     n_slices=N_SLICES,\n",
    "#     min_liver_pixels=0 # Keep all slices for training\n",
    "# )\n",
    "\n",
    "# val_metadata = pre_extract_slices_to_disk(\n",
    "#     volume_list=val_volumes,\n",
    "#     output_dir=\"./preprocessed_data/val\",\n",
    "#     n_slices=N_SLICES,\n",
    "#     min_liver_pixels=0  # Keep all slices for validation\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:27:22.556820Z",
     "iopub.status.busy": "2025-11-23T00:27:22.556628Z",
     "iopub.status.idle": "2025-11-23T00:27:22.577830Z",
     "shell.execute_reply": "2025-11-23T00:27:22.577283Z",
     "shell.execute_reply.started": "2025-11-23T00:27:22.556804Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil, os\n",
    "\n",
    "# shutil.make_archive(\"preprocessed_data\", \"zip\", \"/kaggle/working\", \"/kaggle/working/preprocessed_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T09:48:47.014587Z",
     "iopub.status.busy": "2025-11-23T09:48:47.013979Z",
     "iopub.status.idle": "2025-11-23T09:48:47.023271Z",
     "shell.execute_reply": "2025-11-23T09:48:47.022441Z",
     "shell.execute_reply.started": "2025-11-23T09:48:47.014557Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PreExtractedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for pre-extracted slices\n",
    "    Super fast loading!\n",
    "    \"\"\"\n",
    "    def __init__(self, metadata_csv, transform=None):\n",
    "        self.metadata = pd.read_csv(metadata_csv)\n",
    "        self.transform = transform\n",
    "        self.base_dir = os.path.dirname(metadata_csv)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        \n",
    "        # Extract relative filenames (basename)\n",
    "        img_name = os.path.basename(row['image_path'])\n",
    "        mask_name = os.path.basename(row['mask_path'])\n",
    "\n",
    "        # Construct correct paths on Kaggle\n",
    "        image_path = os.path.join(self.base_dir, \"images\", img_name)\n",
    "        mask_path = os.path.join(self.base_dir, \"masks\", mask_name)\n",
    "\n",
    "        # Load\n",
    "        image = np.load(image_path)['data'].astype(np.float32)\n",
    "        mask = np.load(mask_path)['data'].astype(np.float32)\n",
    "\n",
    "        # Transform\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        mask = mask.unsqueeze(0)\n",
    "        return image, mask\n",
    "\n",
    "def get_train_transforms():\n",
    "    \"\"\"Training augmentation pipeline\"\"\"\n",
    "    return A.Compose([\n",
    "        #A.Resize(height=256, width=256, interpolation=cv2.INTER_LINEAR, mask_interpolation=cv2.INTER_NEAREST), # Only resize if needed\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, border_mode=cv2.BORDER_CONSTANT, p=0.5),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    \"\"\"Validation transform (no augmentation)\"\"\"\n",
    "    return A.Compose([\n",
    "        #A.Resize(height=256, width=256, interpolation=cv2.INTER_LINEAR, mask_interpolation=cv2.INTER_NEAREST), # Only resize if needed\n",
    "        ToTensorV2()])\n",
    "\n",
    "print(\"‚úÖ Dataset and transforms defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T05:51:10.858959Z",
     "iopub.status.busy": "2025-11-23T05:51:10.858712Z",
     "iopub.status.idle": "2025-11-23T05:51:10.899339Z",
     "shell.execute_reply": "2025-11-23T05:51:10.898577Z",
     "shell.execute_reply.started": "2025-11-23T05:51:10.858940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_file_paths = sorted(glob.glob(f\"{IMAGE_DIR}/*.nii\"))\n",
    "label_file_paths = sorted(glob.glob(f\"{LABEL_DIR}/*.nii\"))\n",
    "\n",
    "# Create volume list\n",
    "volume_files = [\n",
    "    {\"image\": img_path, \"label\": lbl_path}\n",
    "    for img_path, lbl_path in zip(image_file_paths, label_file_paths)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DATA LOADING\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"‚úÖ Found {len(volume_files)} 3D CT volumes\")\n",
    "\n",
    "### Train/Val split (IMPORTANT: split by volumes, not slices!)\n",
    "train_volumes, val_volumes = train_test_split(\n",
    "    volume_files,\n",
    "    train_size=TRAIN_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"üìä Train volumes: {len(train_volumes)}\")\n",
    "print(f\"üìä Val volumes: {len(val_volumes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T05:51:10.900531Z",
     "iopub.status.busy": "2025-11-23T05:51:10.900161Z",
     "iopub.status.idle": "2025-11-23T05:51:11.285225Z",
     "shell.execute_reply": "2025-11-23T05:51:11.284103Z",
     "shell.execute_reply.started": "2025-11-23T05:51:10.900487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Create datasets\n",
    "train_dataset = PreExtractedDataset(\n",
    "    metadata_csv=\"/kaggle/input/train-preprocess-dataset/train/metadata.csv\",\n",
    "    transform=get_train_transforms()\n",
    ")\n",
    "val_dataset = PreExtractedDataset(\n",
    "    metadata_csv=\"/kaggle/input/preprocess-2d-data/preprocessed_data/val/metadata.csv\",\n",
    "    transform=get_val_transforms()\n",
    ")\n",
    "print(\"‚úÖ Pre-extracted datasets ready!\")\n",
    "\n",
    "### Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Already sorted by volume!\n",
    "    num_workers=NUM_WORKERS, \n",
    "    pin_memory=False,  # Set to False to save memory\n",
    "    drop_last=True,\n",
    "    persistent_workers=False \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DATALOADER INFO\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"‚úÖ Train batches per epoch: {len(train_loader)} | Val batches: {len(val_loader)}\")\n",
    "print(f\"‚úÖ Train slices: {len(train_dataset)} | Val slices: {len(val_dataset)}\")\n",
    "print(f\"‚úÖ Train value range: {len(train_dataset)} | Val slices: {len(val_dataset)}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Init model, optimizer, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:27:23.063333Z",
     "iopub.status.busy": "2025-11-23T00:27:23.063086Z",
     "iopub.status.idle": "2025-11-23T00:27:25.740518Z",
     "shell.execute_reply": "2025-11-23T00:27:25.739741Z",
     "shell.execute_reply.started": "2025-11-23T00:27:23.063315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MODEL SETUP\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "### Define Model: UNet++ with EfficientNet-B5\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS, # ENCODER_WEIGHTS, if using pretrained\n",
    "    in_channels=N_SLICES,  # 2.5D: 3 slices\n",
    "    classes=1,  # Binary segmentation (liver vs background)\n",
    "    activation=None,  # I'll apply sigmoid in loss\n",
    "    decoder_attention_type=\"scse\"  # Spatial-Channel Squeeze & Excitation\n",
    ")\n",
    "\n",
    "# Multi-GPU\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"‚úÖ Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model: {ARCHITECTURE}\")\n",
    "print(f\"‚úÖ Encoder: {ENCODER} (pretrained on {ENCODER_WEIGHTS})\")\n",
    "print(f\"‚úÖ Decoder attention: SCSE\")\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight_dice=0.7, weight_bce=0.3, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight_dice = weight_dice\n",
    "        self.weight_bce = weight_bce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        preds = torch.sigmoid(inputs)\n",
    "        # Compute BCE loss\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets)\n",
    "        \n",
    "        # Compute Dice loss\n",
    "        smooth = 1e-6\n",
    "        intersection = (preds * targets).sum()\n",
    "        union = preds.sum() + targets.sum()\n",
    "        dice_loss = 1 - (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "        # Combined the loss\n",
    "        combined_loss = self.weight_dice * dice_loss + self.weight_bce * bce_loss\n",
    "        return combined_loss\n",
    "\n",
    "\n",
    "loss_fn = DiceBCELoss(weight_dice=0.7, weight_bce=0.3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-7, last_epoch=-1)\n",
    "\n",
    "print(f\"‚úÖ Loss: Combined (Dice + BCE)\")\n",
    "print(f\"‚úÖ Optimizer: AdamW (lr={LR}, weight_decay={WEIGHT_DECAY})\")\n",
    "print(f\"‚úÖ Scheduler: CosineAnnealingLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:27:25.742821Z",
     "iopub.status.busy": "2025-11-23T00:27:25.742340Z",
     "iopub.status.idle": "2025-11-23T00:27:29.044918Z",
     "shell.execute_reply": "2025-11-23T00:27:29.044257Z",
     "shell.execute_reply.started": "2025-11-23T00:27:25.742803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install torchinfo\n",
    "# from torchinfo import summary\n",
    "\n",
    "# summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Configure funtioncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:07:33.962004Z",
     "iopub.status.busy": "2025-11-22T13:07:33.961804Z",
     "iopub.status.idle": "2025-11-22T13:07:33.978792Z",
     "shell.execute_reply": "2025-11-22T13:07:33.978073Z",
     "shell.execute_reply.started": "2025-11-22T13:07:33.961984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def dice_coefficient(pred, target, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"Calculate Dice coefficient for evaluation\"\"\"\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    return dice.item()\n",
    "\n",
    "def iou_score(pred, target, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"Calculate IoU/Jaccard score\"\"\"\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou.item()\n",
    "\n",
    "# ============================================================================\n",
    "# MEMORY OPTIMIZATION UTILITIES\n",
    "# ============================================================================\n",
    "def clear_memory():\n",
    "    \"\"\"Aggressively clear memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "# Monitor memory during training\n",
    "import psutil\n",
    "\n",
    "def monitor_memory_callback():\n",
    "    \"\"\"Callback to monitor memory\"\"\"\n",
    "    process = psutil.Process()\n",
    "    mem_gb = process.memory_info().rss / 1024**3\n",
    "    \n",
    "    if mem_gb > 28:  # Warning if > 28GB (out of 30GB)\n",
    "        print(f\"‚ö†Ô∏è  HIGH MEMORY WARNING: {mem_gb:.2f} GB\")\n",
    "        clear_memory()\n",
    "    return mem_gb\n",
    "\n",
    "# ============================================================================\n",
    "# Training & Validation step\n",
    "# ============================================================================\n",
    "def train_step_optimized(model, loader, loss_fn, optimizer, scaler, device, epoch, accumulation_steps=4):\n",
    "    \"\"\"Training with memory monitoring\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "    running_iou = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"[Epoch {epoch+1}/{EPOCHS}] Training\")\n",
    "    \n",
    "    for batch_idx, (images, masks) in enumerate(pbar):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, masks)\n",
    "            loss = loss / accumulation_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        if (batch_idx+1) % accumulation_steps == 0 or (batch_idx+1) == len(loader):\n",
    "            \n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update weights\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            dice = dice_coefficient(outputs, masks)\n",
    "            iou = iou_score(outputs, masks)\n",
    "        \n",
    "        current_loss = loss.item() * accumulation_steps\n",
    "        running_loss += current_loss\n",
    "        running_dice += dice\n",
    "        running_iou += iou\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{running_loss/num_batches:.4f}\",\n",
    "            'dice': f\"{running_dice/num_batches:.4f}\",\n",
    "            'iou': f\"{running_iou/num_batches:.4f}\"\n",
    "        })\n",
    "        \n",
    "        # Clean up immediately\n",
    "        del images, masks, outputs, loss\n",
    "        \n",
    "        # Monitor memory every 50 batches\n",
    "        if batch_idx % 50 == 0:\n",
    "            mem_gb = monitor_memory_callback()\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{running_loss/num_batches:.4f}\",\n",
    "                'dice': f\"{running_dice/num_batches:.4f}\",\n",
    "                'iou': f\"{running_iou/num_batches:.4f}\",\n",
    "                'mem': f\"{mem_gb:.1f}GB\"\n",
    "            })\n",
    "    \n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_dice = running_dice / num_batches\n",
    "    avg_iou = running_iou / num_batches\n",
    "    \n",
    "    return avg_loss, avg_dice, avg_iou\n",
    "\n",
    "def validation_step_optimized(model, loader, loss_fn, device):\n",
    "    \"\"\"Validation with memory management\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "    running_iou = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        pbar = tqdm(loader, desc=\"Validation\")\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(pbar):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, masks)\n",
    "            \n",
    "            dice = dice_coefficient(outputs, masks)\n",
    "            iou = iou_score(outputs, masks)\n",
    "            \n",
    "            running_loss += loss.detach().cpu().item()\n",
    "            running_dice += dice\n",
    "            running_iou += iou\n",
    "            num_batches += 1\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{running_loss/num_batches:.4f}\",\n",
    "                'dice': f\"{running_dice/num_batches:.4f}\",\n",
    "                'iou': f\"{running_iou/num_batches:.4f}\"\n",
    "            })\n",
    "            \n",
    "            # Clean up\n",
    "            del images, masks, outputs, loss\n",
    "            \n",
    "            # Aggressive memory clearing every 20 batches\n",
    "            if batch_idx % 20 == 0:\n",
    "                clear_memory()\n",
    "    \n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_dice = running_dice / num_batches\n",
    "    avg_iou = running_iou / num_batches\n",
    "    \n",
    "    return avg_loss, avg_dice, avg_iou\n",
    "\n",
    "print(\"‚úÖ Memory-optimized training functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Login wandb & Configure wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:07:33.980023Z",
     "iopub.status.busy": "2025-11-22T13:07:33.979595Z",
     "iopub.status.idle": "2025-11-22T13:07:38.615499Z",
     "shell.execute_reply": "2025-11-22T13:07:38.614580Z",
     "shell.execute_reply.started": "2025-11-22T13:07:33.980008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q wandb\n",
    "!wandb login <<wandb id>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:07:38.617087Z",
     "iopub.status.busy": "2025-11-22T13:07:38.616840Z",
     "iopub.status.idle": "2025-11-22T13:07:49.381262Z",
     "shell.execute_reply": "2025-11-22T13:07:49.380618Z",
     "shell.execute_reply.started": "2025-11-22T13:07:38.617064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CURRENT_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "EXPERIMENT_NAME = f\"{CURRENT_DATE}_{ARCHITECTURE}_{ENCODER}_LR-{LR}_BS-{BATCH_SIZE}_EPOCHS-{EPOCHS}\"\n",
    "if EXTRA_NOTE:\n",
    "    EXPERIMENT_NAME += f\"_{EXTRA_NOTE}\"\n",
    "\n",
    "# Initialize wandb\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    name=EXPERIMENT_NAME,\n",
    "    config={\n",
    "        # Model\n",
    "        \"architecture\": ARCHITECTURE,\n",
    "        \"encoder\": ENCODER,\n",
    "        \"encoder_weights\": ENCODER_WEIGHTS,\n",
    "        \"in_channels\": N_SLICES,\n",
    "        \"decoder_attention\": \"scse\",\n",
    "        \n",
    "        # Data\n",
    "        \"image_size\": IMAGE_SIZE,\n",
    "        \"n_slices\": N_SLICES,\n",
    "        \"hu_window\": [HU_MIN, HU_MAX],\n",
    "        \"train_volumes\": len(train_volumes),\n",
    "        \"val_volumes\": len(val_volumes),\n",
    "        \"train_slices\": len(train_dataset),\n",
    "        \"val_slices\": len(val_dataset),\n",
    "        \n",
    "        # Training\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"CosineAnnealingLR\",\n",
    "        \"loss\": \"Combined (Dice+BCE)\",\n",
    "        \n",
    "        # Other\n",
    "        \"patience\": PATIENCE,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"WANDB INITIALIZED\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"‚úÖ Project: {WANDB_PROJECT}\")\n",
    "print(f\"‚úÖ Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-22T13:09:56.417Z",
     "iopub.execute_input": "2025-11-22T13:07:49.382788Z",
     "iopub.status.busy": "2025-11-22T13:07:49.382251Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STARTING TRAINING (MEMORY OPTIMIZED)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Initialize\n",
    "scaler = GradScaler()\n",
    "best_val_dice = -1\n",
    "best_epoch = -1\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "accumulation_steps = 4\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch: {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_dice, train_iou = train_step_optimized(model=model, loader=train_loader, loss_fn=loss_fn, optimizer=optimizer, scaler=scaler, device=device, epoch=epoch, accumulation_steps=4)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_dice, val_iou = validation_step_optimized(model=model, loader=val_loader, loss_fn=loss_fn, device=device)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'‚îÄ'*70}\")\n",
    "    print(f\"EPOCH {epoch + 1} SUMMARY\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    print(f\"Train - Loss: {train_loss:.4f} | Dice: {train_dice:.4f}\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f} | Dice: {val_dice:.4f}\")\n",
    "    print(f\"Learning Rate: {current_lr:.7f}\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    run.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"Loss/train\": train_loss,\n",
    "        \"Dice/train\": train_dice,\n",
    "        \"IoU/train\": train_iou,\n",
    "        \"Loss/val\": val_loss,\n",
    "        \"Dice/val\": val_dice,\n",
    "        \"IoU/val\": val_iou,\n",
    "        \"learning_rate\": current_lr,\n",
    "        \"memory_gb\": monitor_memory_callback()\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_dice > best_val_dice:\n",
    "        best_val_dice = val_dice\n",
    "        best_epoch = epoch + 1\n",
    "        epochs_no_improve = 0\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_dice': best_val_dice,\n",
    "            'config': {\n",
    "                'architecture': ARCHITECTURE,\n",
    "                'encoder': ENCODER,\n",
    "                'image_size': IMAGE_SIZE,\n",
    "                'n_slices': N_SLICES,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, \"best_model.pth\")\n",
    "        wandb.save(\"best_model.pth\", policy=\"now\")\n",
    "        \n",
    "        print(f\"‚úÖ New best model saved! Val Dice: {best_val_dice:.4f}\")\n",
    "        print(f\"   Early stopping: {epochs_no_improve}/{PATIENCE}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"‚ö†Ô∏è  No improvement. Early stopping: {epochs_no_improve}/{PATIENCE}\")\n",
    "        \n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"\\n‚ùå EARLY STOPPING at epoch {epoch + 1}\")\n",
    "            print(f\"Best: Epoch {best_epoch}, Dice {best_val_dice:.4f}\")\n",
    "            early_stop = True\n",
    "    \n",
    "    if early_stop:\n",
    "        break\n",
    "    \n",
    "    # Final cleanup for epoch\n",
    "    clear_memory()\n",
    "\n",
    "# Training completed\n",
    "total_training_time = (time() - start_time) / 60\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"‚úÖ Best Val Dice: {best_val_dice:.4f}\")\n",
    "print(f\"‚úÖ Best Epoch: {best_epoch}/{epoch + 1}\")\n",
    "print(f\"‚úÖ Total Training Time: {total_training_time:.2f} minutes\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Update wandb summary\n",
    "run.summary[\"best_val_dice\"] = best_val_dice\n",
    "run.summary[\"best_val_epoch\"] = best_epoch\n",
    "run.summary[\"total_training_time_minutes\"] = total_training_time\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish()\n",
    "\n",
    "# Final cleanup\n",
    "clear_memory()\n",
    "\n",
    "print(\"\\n‚úÖ All done! Check your wandb dashboard for detailed results.\")\n",
    "print(f\"‚úÖ Best model saved as: best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Pre-processing test-set (1 time only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-22T13:09:56.417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def pre_extract_slices_test(volume_list, output_dir, n_slices=3):\n",
    "#     \"\"\"\n",
    "#     Pre-extract slices for TEST set (NO LABELS)\n",
    "#     Output: preprocessed 2.5D images saved to disk + metadata_test.csv\n",
    "    \n",
    "#     Args:\n",
    "#         volume_list: list of {\"image\": path}\n",
    "#         output_dir: folder to save\n",
    "#         n_slices: 2.5D slices count\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     os.makedirs(os.path.join(output_dir, \"images\"), exist_ok=True)\n",
    "\n",
    "#     half_slices = n_slices // 2\n",
    "#     slice_metadata = []\n",
    "\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(\"PREPROCESSING TEST SET\")\n",
    "#     print(f\"{'='*70}\")\n",
    "\n",
    "#     for vol_idx, vol_dict in enumerate(tqdm(volume_list, desc=\"Test Extracting\")):\n",
    "\n",
    "#         # ---- Load CT ----\n",
    "#         img_nii = nib.load(vol_dict['image'])\n",
    "#         img_data = img_nii.get_fdata().astype(np.float32)\n",
    "\n",
    "#         # ---- Apply same HU normalization as train/val ----\n",
    "#         img_data = np.clip(img_data, HU_MIN, HU_MAX)\n",
    "#         img_data = (img_data - HU_MIN) / (HU_MAX - HU_MIN)\n",
    "\n",
    "#         H, W, Z = img_data.shape\n",
    "\n",
    "#         for z_idx in range(Z):\n",
    "\n",
    "#             # ---- Build 2.5D slices (e.g. 3 slices: z-1, z, z+1) ----\n",
    "#             slices_list = []\n",
    "#             for offset in range(-half_slices, half_slices + 1):\n",
    "#                 z = np.clip(z_idx + offset, 0, Z - 1)\n",
    "#                 slices_list.append(img_data[:, :, z])\n",
    "\n",
    "#             image_25d = np.stack(slices_list, axis=-1)   # (H, W, 3)\n",
    "\n",
    "#             # ---- Resize (same as train/val) ----\n",
    "#             image_25d = cv2.resize(\n",
    "#                 image_25d,\n",
    "#                 (IMAGE_SIZE, IMAGE_SIZE),\n",
    "#                 interpolation=cv2.INTER_LINEAR\n",
    "#             )\n",
    "\n",
    "#             # ---- Save ----\n",
    "#             slice_id = f\"vol{vol_idx:03d}_slice{z_idx:03d}\"\n",
    "#             img_path = os.path.join(output_dir, \"images\", f\"{slice_id}.npz\")\n",
    "#             np.savez_compressed(img_path, data=image_25d)\n",
    "\n",
    "#             slice_metadata.append({\n",
    "#                 'image_path': img_path,\n",
    "#                 'volume_id': vol_idx,\n",
    "#                 'slice_id': slice_id,\n",
    "#                 'orig_H': H,\n",
    "#                 'orig_W': W\n",
    "#                 'z_idx': z_idx,\n",
    "#             })\n",
    "\n",
    "#         del img_data\n",
    "#         gc.collect()\n",
    "\n",
    "#     # ---- Save metadata ----\n",
    "#     metadata_path = os.path.join(output_dir, \"metadata_test.csv\")\n",
    "#     pd.DataFrame(slice_metadata).to_csv(metadata_path, index=False)\n",
    "\n",
    "#     print(f\"‚úÖ Extracted {len(slice_metadata)} test slices\")\n",
    "#     print(f\"üìÑ Metadata saved at: {metadata_path}\")\n",
    "\n",
    "#     return metadata_path\n",
    "\n",
    "# test_image_paths = sorted(glob.glob(\"/kaggle/input/aio2025liverseg/test/*.nii\"))\n",
    "\n",
    "# test_volumes = [\n",
    "#     {\"image\": img_path}\n",
    "#     for img_path in test_image_paths\n",
    "# ]\n",
    "\n",
    "# metadata_test_csv = pre_extract_slices_test(\n",
    "#     volume_list=test_volumes,\n",
    "#     output_dir=\"./preprocessed_data/test\",\n",
    "#     n_slices=N_SLICES\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Load Data + Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:47:43.203167Z",
     "iopub.status.busy": "2025-11-23T13:47:43.202778Z",
     "iopub.status.idle": "2025-11-23T13:47:43.326979Z",
     "shell.execute_reply": "2025-11-23T13:47:43.326087Z",
     "shell.execute_reply.started": "2025-11-23T13:47:43.203140Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Post-processing functions loaded\n"
     ]
    }
   ],
   "source": [
    "def keep_largest_component_3d(mask_3d):\n",
    "    \"\"\"Keep only largest connected component\"\"\"\n",
    "    if mask_3d.sum() == 0:\n",
    "        return mask_3d\n",
    "    \n",
    "    labeled = label(mask_3d, connectivity=3)\n",
    "    if labeled.max() == 0:\n",
    "        return mask_3d\n",
    "    \n",
    "    component_sizes = np.bincount(labeled.ravel())\n",
    "    component_sizes[0] = 0\n",
    "    largest_id = component_sizes.argmax()\n",
    "    \n",
    "    return (labeled == largest_id).astype(np.uint8)\n",
    "\n",
    "def fill_holes_3d(mask_3d):\n",
    "    \"\"\"Fill holes slice-by-slice\"\"\"\n",
    "    result = mask_3d.copy()\n",
    "    for z in range(mask_3d.shape[2]):\n",
    "        result[:, :, z] = binary_fill_holes(mask_3d[:, :, z]).astype(np.uint8)\n",
    "    return result\n",
    "\n",
    "def postprocess_volume(pred_volume, min_size=2000):\n",
    "    \"\"\"Complete post-processing pipeline\"\"\"\n",
    "    # Remove small objects\n",
    "    mask = remove_small_objects(\n",
    "        pred_volume.astype(bool),\n",
    "        min_size=min_size,\n",
    "        connectivity=3\n",
    "    ).astype(np.uint8)\n",
    "    \n",
    "    # Keep largest component\n",
    "    mask = keep_largest_component_3d(mask)\n",
    "    \n",
    "    # Fill holes\n",
    "    mask = fill_holes_3d(mask)\n",
    "    \n",
    "    return mask\n",
    "    \n",
    "class PreExtractedTestDataset(Dataset):\n",
    "    def __init__(self, metadata_csv, transform=None):\n",
    "        df = pd.read_csv(metadata_csv)\n",
    "        df[\"image_path\"] = df[\"image_path\"].apply(lambda x: os.path.abspath(x))\n",
    "        self.metadata = df            \n",
    "        self.transform = transform\n",
    "        self.base_dir = os.path.dirname(metadata_csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        img_name = os.path.basename(row['image_path'])\n",
    "        image_path = os.path.join(self.base_dir, \"images\", img_name)\n",
    "\n",
    "        image = np.load(image_path)['data'].astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"slice_id\": row[\"slice_id\"],\n",
    "            \"volume_id\": row[\"volume_id\"],\n",
    "            \"z_idx\": row[\"z_idx\"]\n",
    "        }\n",
    "\n",
    "def rle_encode(mask: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Run-length encode a 2D binary mask (1 for foreground, 0 for background)\n",
    "    Empty mask -> '1 0'.\n",
    "    \"\"\"\n",
    "    assert mask.ndim == 2, \"rle_encode expects a 2D mask\"\n",
    "    pixels = mask.astype(np.uint8).flatten(order='F')  # column-major\n",
    "    if pixels.max() == 0:\n",
    "        return \"1 0\"\n",
    "    # Pad with zeros at both ends to catch transitions cleanly\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return \" \".join(map(str, runs))\n",
    "\n",
    "def test_transforms():\n",
    "    \"\"\"Testing transform (no augmentation)\"\"\"\n",
    "    return A.Compose([\n",
    "        # A.Resize(height=256, width=256, interpolation=cv2.INTER_LINEAR, mask_interpolation=cv2.INTER_NEAREST),\n",
    "        ToTensorV2()])\n",
    "\n",
    "# ============ TTA FUNCTIONS ============\n",
    "import cv2\n",
    "\n",
    "def apply_tta_transform(image, tta_type):\n",
    "    \"\"\"Apply TTA transformation to image\"\"\"\n",
    "    if tta_type == \"hflip\":\n",
    "        return np.flip(image, axis=2).copy()  # flip along width (axis=2 for CHW format)\n",
    "    elif tta_type == \"rot5\":\n",
    "        return rotate_image(image, 5)\n",
    "    elif tta_type == \"rot-5\":\n",
    "        return rotate_image(image, -5)\n",
    "    else:  # original\n",
    "        return image\n",
    "\n",
    "def reverse_tta_transform(mask, tta_type):\n",
    "    \"\"\"Reverse TTA transformation on predicted mask (HW format)\"\"\"\n",
    "    if tta_type == \"hflip\":\n",
    "        return np.flip(mask, axis=1).copy()  # flip along width\n",
    "    elif tta_type == \"rot5\":\n",
    "        return rotate_image_2d(mask, -5)  # rotate back\n",
    "    elif tta_type == \"rot-5\":\n",
    "        return rotate_image_2d(mask, 5)\n",
    "    else:  # original\n",
    "        return mask\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    \"\"\"Rotate CHW image by angle (degrees)\"\"\"\n",
    "    C, H, W = image.shape\n",
    "    result = np.zeros_like(image)\n",
    "    for c in range(C):\n",
    "        center = (W // 2, H // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        result[c] = cv2.warpAffine(image[c], M, (W, H), flags=cv2.INTER_LINEAR)\n",
    "    return result\n",
    "\n",
    "def rotate_image_2d(image, angle):\n",
    "    \"\"\"Rotate 2D image by angle (degrees)\"\"\"\n",
    "    H, W = image.shape\n",
    "    center = (W // 2, H // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    return cv2.warpAffine(image, M, (W, H), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "print(\"‚úÖ Post-processing functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:47:53.192332Z",
     "iopub.status.busy": "2025-11-23T13:47:53.191325Z",
     "iopub.status.idle": "2025-11-23T13:47:57.377473Z",
     "shell.execute_reply": "2025-11-23T13:47:57.376580Z",
     "shell.execute_reply.started": "2025-11-23T13:47:53.192288Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING BEST MODEL\n",
      "======================================================================\n",
      "‚úÖ Model loaded successfully!\n",
      "‚úÖ Best Val Dice from training: 0.9584\n",
      "‚úÖ Trained at epoch: 21\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"LOADING BEST MODEL\")\n",
    "print(f\"{'='*70}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"/kaggle/input/unetplusplus-512x512/pytorch/default/3/2d_unetplusplus_epoch37_size512\", map_location=\"cpu\")\n",
    "\n",
    "# Recreate model\n",
    "inference_model = smp.UnetPlusPlus(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=None,\n",
    "    in_channels=N_SLICES,\n",
    "    classes=1,\n",
    "    activation=None,\n",
    "    decoder_attention_type=\"scse\"\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "inference_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "inference_model = inference_model.to(device)\n",
    "inference_model.eval()\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"‚úÖ Best Val Dice from training: {checkpoint['best_val_dice']:.4f}\")\n",
    "print(f\"‚úÖ Trained at epoch: {checkpoint['epoch']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T11:16:04.711293Z",
     "iopub.status.busy": "2025-11-23T11:16:04.711014Z",
     "iopub.status.idle": "2025-11-23T11:21:38.887438Z",
     "shell.execute_reply": "2025-11-23T11:21:38.886301Z",
     "shell.execute_reply.started": "2025-11-23T11:16:04.711272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---- Load metadata ----\n",
    "metadata_csv=\"/kaggle/input/preprocessed-data/preprocessed_data/test/metadata_test.csv\" \n",
    "TEST_DIR = \"/kaggle/input/aio2025liverseg/test\"\n",
    "test_volume_paths = sorted(glob.glob(f\"{TEST_DIR}/*.nii\"))\n",
    "df = pd.read_csv(metadata_csv)\n",
    "\n",
    "# ---- Load original volume shapes ----\n",
    "original_shapes = {}\n",
    "for vol_id, grp in pd.read_csv(metadata_csv).groupby(\"volume_id\"):\n",
    "    H = int(grp[\"orig_H\"].iloc[0])\n",
    "    W = int(grp[\"orig_W\"].iloc[0])\n",
    "    Z = int(grp[\"z_idx\"].max()) + 1\n",
    "    original_shapes[int(vol_id)] = (H, W, Z)\n",
    "\n",
    "# ---- Dataset & Loader ----\n",
    "test_ds = PreExtractedTestDataset(metadata_csv, transform=test_transforms())\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# ---- Storage ----\n",
    "pred_masks = {}   # pred_masks[vol_id][z] = 2D mask original size\n",
    "\n",
    "for vol_id in range(len(test_volume_paths)):\n",
    "    pred_masks[vol_id] = {}\n",
    "\n",
    "# ---- TTA CONFIG ----\n",
    "TTA_TRANSFORMS = [\"original\", \"hflip\", \"rot5\"]  # 3 augmentations\n",
    "\n",
    "# ---- Inference WITH TTA ----\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"RUNNING INFERENCE WITH TTA ({len(TTA_TRANSFORMS)} transforms)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        imgs = batch[\"image\"].to(device)  # (B,3,H,W)\n",
    "        B = imgs.shape[0]\n",
    "        \n",
    "        # Accumulate predictions from all TTA transforms\n",
    "        prob_sum = torch.zeros((B, 1, imgs.shape[2], imgs.shape[3]), device=device)\n",
    "        \n",
    "        for tta_type in TTA_TRANSFORMS:\n",
    "            # Apply TTA transform\n",
    "            imgs_tta = torch.stack([\n",
    "                torch.from_numpy(apply_tta_transform(imgs[i].cpu().numpy(), tta_type))\n",
    "                for i in range(B)\n",
    "            ]).to(device)\n",
    "            \n",
    "            # Predict\n",
    "            logits = inference_model(imgs_tta)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            # Reverse TTA on predictions\n",
    "            probs_reversed = torch.stack([\n",
    "                torch.from_numpy(reverse_tta_transform(probs[i, 0].cpu().numpy(), tta_type))\n",
    "                for i in range(B)\n",
    "            ]).unsqueeze(1).to(device)\n",
    "            \n",
    "            prob_sum += probs_reversed\n",
    "        \n",
    "        # Average predictions\n",
    "        probs_avg = prob_sum / len(TTA_TRANSFORMS)\n",
    "        preds = (probs_avg > 0.5).float().cpu().numpy()\n",
    "\n",
    "        # Loop t·ª´ng slice\n",
    "        for i in range(len(preds)):\n",
    "            pred_resized = preds[i, 0, :, :]  # (IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "            vol_id = int(batch[\"volume_id\"][i])\n",
    "            z = int(batch[\"z_idx\"][i])\n",
    "\n",
    "            orig_H, orig_W, _ = original_shapes[vol_id]\n",
    "\n",
    "            # ---- Resize back to original size ----\n",
    "            mask_original = cv2.resize(\n",
    "                pred_resized.astype(np.uint8),\n",
    "                (orig_W, orig_H),\n",
    "                interpolation=cv2.INTER_NEAREST\n",
    "            )\n",
    "\n",
    "            pred_masks[vol_id][z] = mask_original\n",
    "\n",
    "# ---- POST-PROCESSING (NEW!) ----\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPLYING POST-PROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for vol_id in tqdm(range(len(test_volume_paths)), desc=\"Post-processing\"):\n",
    "    H, W, Z = original_shapes[vol_id]\n",
    "    \n",
    "    # Stack slices into 3D volume\n",
    "    pred_volume = np.zeros((H, W, Z), dtype=np.uint8)\n",
    "    for z in range(Z):\n",
    "        pred_volume[:, :, z] = pred_masks[vol_id][z]\n",
    "    \n",
    "    # Apply post-processing\n",
    "    pred_volume_cleaned = postprocess_volume(pred_volume, min_size=1000)\n",
    "    \n",
    "    # Update pred_masks with cleaned predictions\n",
    "    for z in range(Z):\n",
    "        pred_masks[vol_id][z] = pred_volume_cleaned[:, :, z]\n",
    "\n",
    "print(\"‚úÖ Post-processing completed!\")\n",
    "\n",
    "# ---- Create CSV ----\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING SUBMISSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "submission = []\n",
    "for vol_id in range(len(test_volume_paths)):\n",
    "    Z = original_shapes[vol_id][2]\n",
    "\n",
    "    for z in range(Z):\n",
    "        mask = pred_masks[vol_id][z]\n",
    "        rle = rle_encode(mask)\n",
    "        id_str = f\"cell_{vol_id+80}_{z}\"\n",
    "        submission.append([id_str, rle])\n",
    "\n",
    "# ---- Save CSV ----\n",
    "df_sub = pd.DataFrame(submission, columns=[\"id\", \"rle\"])\n",
    "df_sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ submission.csv created with TTA!\")\n",
    "print(f\"‚úÖ Total predictions: {len(submission)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize random prediction on Val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:08:48.871344Z",
     "iopub.status.busy": "2025-11-23T07:08:48.871014Z",
     "iopub.status.idle": "2025-11-23T07:08:55.572848Z",
     "shell.execute_reply": "2025-11-23T07:08:55.571569Z",
     "shell.execute_reply.started": "2025-11-23T07:08:48.871304Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# random.seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = inference_model\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "VAL_DIR = \"/kaggle/input/preprocess-2d-data/preprocessed_data/val\"\n",
    "val_image_paths = sorted(glob.glob(f\"{VAL_DIR}/images/*.npz\"))\n",
    "val_mask_paths = sorted(glob.glob(f\"{VAL_DIR}/masks/*.npz\"))\n",
    "total_image = len(val_image_paths)\n",
    "\n",
    "random_indices = random.sample(range(total_image), k=10)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4 * len(random_indices)))\n",
    "\n",
    "for index, random_index in enumerate(random_indices):\n",
    "    image = np.load(val_image_paths[random_index])[\"data\"].astype(np.float32)\n",
    "    mask = np.load(val_mask_paths[random_index])[\"data\"].astype(np.float32)\n",
    "\n",
    "    original_image = image[:, :, 1]\n",
    "\n",
    "    # Convert image to tensor\n",
    "    image_tensor = ToTensorV2()(image=image)[\"image\"] # (1, H, W)\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        logits = model(image_tensor)\n",
    "        pred_image = (torch.sigmoid(logits).cpu().numpy() > 0.5).astype(np.uint8).squeeze()\n",
    "\n",
    "    # Visualize\n",
    "    ax1 = plt.subplot(len(random_indices), 3, index*3 + 1)\n",
    "    ax1.imshow(original_image, cmap=\"gray\")\n",
    "    ax1.imshow(mask, cmap=\"Blues\", alpha=0.5)\n",
    "    ax1.set_title(f\"Mask/Image, image: {random_index}\", fontsize=13)\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2 = plt.subplot(len(random_indices), 3, index*3 + 2)\n",
    "    ax2.imshow(original_image, cmap=\"gray\")\n",
    "    ax2.imshow(pred_image, cmap=\"Reds\", alpha=0.5)\n",
    "    ax2.set_title(\"Prediction/Image\", fontsize=13)\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    ax3 = plt.subplot(len(random_indices), 3, index*3 + 3)\n",
    "    fp = np.logical_and(pred_image == 1, mask == 0)   # False Positive\n",
    "    fn = np.logical_and(pred_image == 0, mask == 1)   # False Negative\n",
    "    ax3.imshow(fp, cmap=\"Blues\", alpha=fp * 0.8)\n",
    "    ax3.imshow(fn, cmap=\"Reds\", alpha=fn * 0.8)\n",
    "    ax3.set_title(\"Error Map (FP=Red, FN=Blue)\", fontsize=13)\n",
    "    ax3.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize k random slices by 4 bins (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:33:58.335738Z",
     "iopub.status.busy": "2025-11-23T07:33:58.335353Z",
     "iopub.status.idle": "2025-11-23T07:34:20.398044Z",
     "shell.execute_reply": "2025-11-23T07:34:20.396817Z",
     "shell.execute_reply.started": "2025-11-23T07:33:58.335713Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, glob, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ==================== CONFIG =====================\n",
    "VAL_DIR = \"/kaggle/input/preprocess-2d-data/preprocessed_data/val\"\n",
    "K = 5   # s·ªë ·∫£nh mu·ªën xem cho m·ªói bin\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = inference_model.to(DEVICE).eval()\n",
    "# =================================================\n",
    "\n",
    "\n",
    "def dice_score(mask, pred):\n",
    "    mask = mask.astype(bool)\n",
    "    pred = pred.astype(bool)\n",
    "\n",
    "    if mask.sum() == 0 and pred.sum() == 0:\n",
    "        return 1.0\n",
    "    if mask.sum() == 0 and pred.sum() > 0:\n",
    "        return 0.0\n",
    "\n",
    "    inter = (mask & pred).sum()\n",
    "    dice = 2 * inter / (mask.sum() + pred.sum())\n",
    "    return float(dice)\n",
    "\n",
    "\n",
    "# ===== Read val slices =====\n",
    "val_images = sorted(glob.glob(f\"{VAL_DIR}/images/*.npz\"))\n",
    "val_masks = sorted(glob.glob(f\"{VAL_DIR}/masks/*.npz\"))\n",
    "\n",
    "N = len(val_images)\n",
    "foreground_ratio = []\n",
    "\n",
    "for i in range(N):\n",
    "    m = np.load(val_masks[i])[\"data\"].astype(np.float32)\n",
    "    fg = (m > 0).mean()\n",
    "    foreground_ratio.append(fg)\n",
    "\n",
    "foreground_ratio = np.array(foreground_ratio)\n",
    "\n",
    "# T√≠nh 4 bin theo quartile\n",
    "q1, q2, q3 = np.percentile(foreground_ratio, [25, 50, 75])\n",
    "\n",
    "bins = [\n",
    "    np.where(foreground_ratio <= q1)[0],                        # Bin 1\n",
    "    np.where((foreground_ratio > q1) & (foreground_ratio <= q2))[0],  # Bin 2\n",
    "    np.where((foreground_ratio > q2) & (foreground_ratio <= q3))[0],  # Bin 3\n",
    "    np.where(foreground_ratio > q3)[0],                         # Bin 4\n",
    "]\n",
    "\n",
    "bin_names = [\"Low FG\", \"Medium-Low\", \"Medium-High\", \"High FG\"]\n",
    "\n",
    "# ==================== Visualization =====================\n",
    "for b_idx, indices in enumerate(bins):\n",
    "    if len(indices) == 0:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Bin {b_idx+1} ({bin_names[b_idx]}) ‚Äî showing {K} random slices ===\")\n",
    "\n",
    "    chosen = random.sample(list(indices), min(K, len(indices)))\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 4 * len(chosen)))\n",
    "\n",
    "    for j, idx in enumerate(chosen):\n",
    "\n",
    "        img = np.load(val_images[idx])[\"data\"].astype(np.float32)\n",
    "        mask = np.load(val_masks[idx])[\"data\"].astype(np.float32)\n",
    "        original = img[:, :, 1]  # slice middle n·∫øu l√† 3-channel\n",
    "\n",
    "        # tensor\n",
    "        tensor = ToTensorV2()(image=img)[\"image\"].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            logit = model(tensor)\n",
    "            pred = (torch.sigmoid(logit).cpu().numpy() > 0.5).astype(np.uint8).squeeze()\n",
    "\n",
    "        dice = dice_score(mask, pred)\n",
    "\n",
    "        # ==== Plot layout: mask, pred, error ====\n",
    "        ax1 = plt.subplot(len(chosen), 3, j*3 + 1)\n",
    "        ax1.imshow(original, cmap=\"gray\")\n",
    "        ax1.imshow(mask, cmap=\"Blues\", alpha=0.5)\n",
    "        ax1.set_title(f\"Mask/Image (idx={idx})\")\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "        ax2 = plt.subplot(len(chosen), 3, j*3 + 2)\n",
    "        ax2.imshow(original, cmap=\"gray\")\n",
    "        ax2.imshow(pred, cmap=\"Reds\", alpha=0.5)\n",
    "        ax2.set_title(f\"Prediction/Image\\nDice={dice:.4f}\")\n",
    "        ax2.axis(\"off\")\n",
    "\n",
    "        # Error map\n",
    "        fp = (pred == 1) & (mask == 0)\n",
    "        fn = (pred == 0) & (mask == 1)\n",
    "\n",
    "        ax3 = plt.subplot(len(chosen), 3, j*3 + 3)\n",
    "        ax3.imshow(fp, cmap=\"Reds\", alpha=fp*0.8)\n",
    "        ax3.imshow(fn, cmap=\"Blues\", alpha=fn*0.8)\n",
    "        ax3.set_title(\"Error Map\")\n",
    "        ax3.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize top K bad predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:24:48.856997Z",
     "iopub.status.busy": "2025-11-23T07:24:48.856655Z",
     "iopub.status.idle": "2025-11-23T07:31:44.114950Z",
     "shell.execute_reply": "2025-11-23T07:31:44.113872Z",
     "shell.execute_reply.started": "2025-11-23T07:24:48.856976Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import glob\n",
    "from tdqm.auto import tdqm\n",
    "\n",
    "def dice_score(mask, pred):\n",
    "    mask = mask.astype(bool)\n",
    "    pred = pred.astype(bool)\n",
    "\n",
    "    if mask.sum() == 0 and pred.sum() == 0:\n",
    "        return 1.0\n",
    "    if mask.sum() == 0 and pred.sum() > 0:\n",
    "        return 0.0\n",
    "\n",
    "    inter = (mask & pred).sum()\n",
    "    return 2 * inter / (mask.sum() + pred.sum())\n",
    "\n",
    "\n",
    "# ===== Read val slices =====\n",
    "val_images = sorted(glob.glob(f\"{VAL_DIR}/images/*.npz\"))\n",
    "val_masks = sorted(glob.glob(f\"{VAL_DIR}/masks/*.npz\"))\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "scores = []\n",
    "\n",
    "# Compute Dice for entire valset\n",
    "for idx in tdqm(range(len(val_images)), desc=\"Inference in Validation data\"):\n",
    "    img = np.load(val_images[idx])[\"data\"].astype(np.float32)\n",
    "    mask = np.load(val_masks[idx])[\"data\"].astype(np.float32)\n",
    "\n",
    "    tensor = ToTensorV2()(image=img)[\"image\"].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        logit = model(tensor)\n",
    "        pred = (torch.sigmoid(logit).cpu().numpy() > 0.5).astype(np.uint8).squeeze()\n",
    "\n",
    "    dice = dice_score(mask, pred)\n",
    "    scores.append((dice, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:36:06.621422Z",
     "iopub.status.busy": "2025-11-23T07:36:06.620670Z",
     "iopub.status.idle": "2025-11-23T07:36:21.543802Z",
     "shell.execute_reply": "2025-11-23T07:36:21.542902Z",
     "shell.execute_reply.started": "2025-11-23T07:36:06.621395Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== CONFIG ======================\n",
    "K = 30  # The number of bad predictions that we want to visualize\n",
    "# =================================================\n",
    "\n",
    "# Sort by Dice ascending (worst first)\n",
    "scores = sorted(scores, key=lambda x: x[0])\n",
    "\n",
    "# Take top K worst\n",
    "worst = scores[:K]\n",
    "\n",
    "print(\"\\n=== Worst K slices by Dice ===\")\n",
    "for d, idx in worst:\n",
    "    print(f\"idx={idx}, Dice={d:.4f}\")\n",
    "# ================= Visualization ==================\n",
    "fig = plt.figure(figsize=(12, 4 * K))\n",
    "\n",
    "for j, (d, idx) in enumerate(worst):\n",
    "    img = np.load(val_images[idx])[\"data\"].astype(np.float32)\n",
    "    mask = np.load(val_masks[idx])[\"data\"].astype(np.float32)\n",
    "    original = img[:, :, 1]\n",
    "\n",
    "    tensor = ToTensorV2()(image=img)[\"image\"].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        logit = model(tensor)\n",
    "        pred = (torch.sigmoid(logit).cpu().numpy() > 0.5).astype(np.uint8).squeeze()\n",
    "\n",
    "    # Plot\n",
    "    ax1 = plt.subplot(K, 3, j*3 + 1)\n",
    "    ax1.imshow(original, cmap=\"gray\")\n",
    "    ax1.imshow(mask, cmap=\"Blues\", alpha=0.5)\n",
    "    ax1.set_title(f\"Mask/Image (idx={idx})\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2 = plt.subplot(K, 3, j*3 + 2)\n",
    "    ax2.imshow(original, cmap=\"gray\")\n",
    "    ax2.imshow(pred, cmap=\"Reds\", alpha=0.5)\n",
    "    ax2.set_title(f\"Prediction/Image\\nDice={d:.4f}\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    fp = (pred == 1) & (mask == 0)\n",
    "    fn = (pred == 0) & (mask == 1)\n",
    "\n",
    "    ax3 = plt.subplot(K, 3, j*3 + 3)\n",
    "    ax3.imshow(fp, cmap=\"Reds\", alpha=fp*0.8)\n",
    "    ax3.imshow(fn, cmap=\"Blues\", alpha=fn*0.8)\n",
    "    ax3.set_title(\"Error Map\")\n",
    "    ax3.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13423659,
     "isSourceIdPinned": false,
     "sourceId": 110174,
     "sourceType": "competition"
    },
    {
     "datasetId": 8795147,
     "sourceId": 13812427,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8797652,
     "sourceId": 13815676,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8805635,
     "sourceId": 13826771,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 512245,
     "modelInstanceId": 496879,
     "sourceId": 657316,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 512251,
     "modelInstanceId": 496887,
     "sourceId": 657915,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
